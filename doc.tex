\documentclass[a4paper,14pt,russian]{extreport}

\usepackage{extsizes}
\usepackage{cmap}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{pscyr}
\usepackage{indentfirst}
\renewcommand{\rmdefault}{ftm}
\frenchspacing
\usepackage[nodisplayskipstretch]{setspace}
\onehalfspacing

\usepackage{graphicx}
\graphicspath{ {images/} } 

\usepackage{amssymb,amsfonts,amsmath,amsthm}
\usepackage[usenames,dvipsnames]{color}
\usepackage{makecell}
\usepackage{multirow}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{syntax}
\usepackage{listings}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[C]{\thepage}
\fancyheadoffset{0mm}
\fancyfootoffset{0mm}
\setlength{\headheight}{17pt}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\fancypagestyle{plain}{ 
	\fancyhf{}
	\chead{\thepage}}
\setcounter{page}{4}

\renewcommand{\arraystretch}{1.4}
\setlength{\floatsep}{2em}

\usepackage[tableposition=top]{caption}
\usepackage{subcaption}
\DeclareCaptionLabelFormat{gostfigurelabel}{Рисунок #2}
\DeclareCaptionLabelFormat{gosttablelabel}{Таблица #2}
\DeclareCaptionLabelSeparator{gost}{~---~}
\captionsetup{labelsep=gost}
\captionsetup[figure]{labelformat=gostfigurelabel}
\captionsetup[table]
	{labelformat=gosttablelabel, justification=raggedright, singlelinecheck = false}
\usepackage{threeparttable}
\renewcommand{\thesubfigure}{\asbuk{subfigure}}

\usepackage{cleveref}

\usepackage{geometry}
\geometry{left=2.5cm}
\geometry{right=1.0cm}
\geometry{top=2.0cm}
\geometry{bottom=2.0cm}

	\usepackage{titlesec}
	 
	\titleformat{\chapter}
		{\filcenter\bfseries}
		{\thechapter}
		{8pt}
		{\MakeUppercase}
	 
	\titleformat{\section}
		{\normalsize\bfseries}
		{\thesection}
		{1em}{}
	 
	\titleformat{\subsection}
		{\normalsize\bfseries}
		{\thesubsection}
		{1em}{}
	
	\titlespacing*{\chapter}{0pt}{-30pt}{8pt}
	\titlespacing*{\section}{\parindent}{*4}{*4}
	\titlespacing*{\subsection}{\parindent}{*4}{*4}
	
	\usepackage{enumitem}
	\makeatletter
	    \AddEnumerateCounter{\asbuk}{\@asbuk}{м)}
	\makeatother
	\renewcommand{\labelitemi}{-}
	\renewcommand{\labelenumi}{\arabic{enumi}.}
	\renewcommand{\labelenumii}{\asbuk{enumii}.}
	
	\usepackage{tocloft}
	\renewcommand{\cfttoctitlefont}{\hspace{0.38\textwidth} \bfseries\MakeUppercase}
	\renewcommand{\cftaftertoctitleskip}{2em}
	\renewcommand{\cftbeforetoctitleskip}{-1em}
	\renewcommand{\cftchapfont}{\normalsize\bfseries}
	\renewcommand{\cftdotsep}{1}
	\setcounter{tocdepth}{2}
	
	\setlength\cftbeforechapskip{1em}
	\setlength\cftbeforesecskip{.5em}
	\setlength\cftbeforesubsecskip{.25em}
	
	\newcommand{\empline}{\vspace{1em}}
	\newcommand{\likechapterheading}[1]{ 
		\clearpage   
		\begin{center}
		\textbf{\MakeUppercase{#1}}
		\end{center}
	}

\usepackage{etoolbox}
	\makeatletter
		\patchcmd{\l@chapter}{#1}{\MakeUppercase{#1}}{}{}
		\renewcommand{\@dotsep}{1}
		\newcommand{\l@likechapter}[2]
			{{\bfseries\@dottedtocline{0}{0pt}{0pt}{#1}{\bfseries#2}}}
	\makeatother
	\newcommand{\likechapter}[1]{ 
		\likechapterheading{#1}    
		\addtocontents{toc}{\vspace{1em}}
		\addcontentsline{toc}{likechapter}{\MakeUppercase{#1}}
		\empline
	}
	
	\usepackage[
		backend=biber,
		bibencoding=utf8,
		sorting=none,
		style=gost-numeric,
		autolang=none,
		eprint=false
	]{biblatex}
	\addbibresource{references/literature.bib}

\defbibheading{bibheading}[Библиографический список]{
	\likechapterheading{#1}
	\addtocontents{toc}{\vspace{1em}}
	\addcontentsline{toc}{likechapter}{\MakeUppercase{#1}}
}
\DeclareFieldFormat{author}{{#1}}

\usepackage{lastpage}

	\usepackage[title,titletoc]{appendix}
	 
	\titleformat{\paragraph}[display]
		{\filcenter}
		{\MakeUppercase{\chaptertitlename} \thechapter}
		{8pt}
		{\bfseries}{}
	\titlespacing*{\paragraph}{0pt}{-30pt}{8pt}
	 
	\newcommand{\append}[1]{  
		\clearpage
		\stepcounter{chapter}    
		\paragraph{\MakeUppercase{#1}}
		\empline
		\addcontentsline{toc}{likechapter}
					{\MakeUppercase{\chaptertitlename~\Asbuk{chapter}\;#1}}}

\setlength{\parindent}{1.25cm} 
\setlength{\parskip}{6pt}


\newenvironment{gosttable}
	{
		\begin{table}[!h]
			\centering
			\begin{threeparttable}
	}	
	{
			\end{threeparttable}
		\end{table}
	}
	
\newenvironment{eqwhere}
	{где:
		\par\noindent\hspace{2em}\begin{tabular}{>{$}r<{$} @{${}$ -- {}} l}
	}
	{\end{tabular}\par\vspace{\belowdisplayskip}}

\begin{document}

\tableofcontents

\likechapter{Введение}

{\bfseries Актуальность темы исследования.} 
В последнее время большое распространение получили различные 
сервисы-агрегаторы, объединяющие данные из нескольких источников
определённой тематики в один, что уменьшает затрачиваемое пользователем
время на поиск необходимой информации. 

Для обработки запроса от одного пользователя, агрегатору может 
потребоваться совершить десятки или даже сотни запросов к сторонним 
ресурсам. В силу того, что пользователи могут запрашивать одну и ту же 
информацию, самым распространённым способ снижения времени ответа 
является использование промежуточных буферов, так называемых кэшей. Но 
данный способ приводит к снижению актуальности предоставляемых 
агрегатором данных и может быть использован только для предупреждения 
избыточной нагрузки системы. Следовательно, разработка алгоритма 
распределения нестационарной нагрузки, определяющего источник данных 
агрегатора, является актуальной задачей. Однако для её	решения необходим
анализ или прогноз нагрузки системы.

{\bfseries Степень теоретической разработанности темы.}
В открытом доступе существует множество научных работ, описывающих 
методы прогнозирования временных рядов. Но материалы, описывающие 
проблему выбора метода прогнозирования для разработки алгоритма 
распределения нестационарной нагрузки, найти не удалось. Из сказанного выше 
можно сделать вывод о том, что рассматриваемая тема имеет низкую степень 
теоретической проработанности.

{\bfseries Объектом исследования} является прогнозирование нестационарной 
нагрузки сервиса"=агрегатора.

{\bfseries Предметом исследования} являются методы прогнозирования 
временных рядов.

{\bfseries Область исследования.} Проведённое исследование методов 
прогнозирования нагрузки вычислительной системы в пределах разработки 
алгоритма распределения запросов пользователей полностью соответствует 
специальности «Вычислительные машины, комплексы, системы и сети», а 
содержание выпускной квалификационной работы -- техническому
заданию.

{\bfseries Цель и задачи исследования.}  Целью работы является улучшение 
качества обслуживания пользователей сервиса-агрегатора за счет снижения 
среднего времени ответа.

Для достижения данной цели были поставлены следующие задачи:
\begin{enumerate}
	\item Выполнить сравнительный анализ методов прогнозирования 
		временных рядов.
	\item Выбрать наиболее адекватный метод прогнозирования.
	\item Разработать алгоритм распределения нестационарной нагрузки на 
		основе выбранного метода прогнозирования.
	\item Разработать программную реализацию алгоритма.
	\item Выполнить сравнение времени ответа исходной и модернизированной 
		систем.
\end{enumerate}

{\bfseries Теоретическую основу исследования} составляют научные труды
отечественных и зарубежных авторов в области компьютерных технологий и
математической статистики.

{\bfseries Методологическую основу исследования} составляет эксперимент.

{\bfseries Научная новизна работы} заключается в следующем:
\begin{enumerate}
	\item В настоящее время не существует структурированных рекомендаций 
		по выбору метода прогнозирования при разработке алгоритмов 
		распределения нагрузки, представленных в данной работе. 
	\item Разработанный алгоритм распределения нестационарной нагрузки 
		может обеспечить улучшение качества обслуживания пользователей
		сервиса"=агрегатора.
\end{enumerate}

{\bfseries Практическая значимость} данной работы заключается в том, что 
разработанный алгоритм распределения нестационарной нагрузки будет
интегрирован в разрабатываемый сервис"=агрегатор. Кроме того, данный 
алгоритм может быть интегрирован существующими сторонними сервисами, а 
сформулированные рекомендации могут быть использованы для осуществления 
выбора метода прогнозирования при разработке собственного алгоритма 
распределения запросов пользователей.

{\bfseries Апробация результатов исследования.} Сформулированные 
рекомендации по выбору метода прогнозирования обсуждались на VII Конгрессе 
молодых учёных.

{\bfseries Объем и структура работы.} 


\chapter{Сравнительный анализ методов прогнозирования}
\section{Модели прогнозирования AR, MA, ARMA, ARIMA} 
Временной ряд - это ряд наблюдений, произведенных последовательно во 
времени \cite{box2008}. Временным рядом можно назвать огромное количество 
последовательностей, они являются следствием измерений некоторого 
показателя. Например, показатели (характеристики) экономических, природных, 
промышленных, информационных и других систем. 

Существуют модели прогнозирования, позволяющие на основе доступных к 
моменту времени $t$ наблюдений спрогнозировать значение временного ряда в 
определенный момент времени $t+1$ в будущем. Наибольшее же 
распространение получила модель ARIMA (autoregressive integrated moving 
average: интегрированная модель авторегрессии -- скользящего среднего, или 
модель Бокса -- Дженкинса). Она удобна еще и тем, что сочетает в себе модели ARMA, AR и MA, их описание представлено в разделе далее.


\subsection{Модель скользящего среднего MA(q)}
В моделях скользящего среднего текущее значение ряда представляется в виде 
линейной комбинации текущего и прошедших значений ошибки $\varepsilon_t$, 
$\varepsilon_{t-1}$, $...$, $\varepsilon_{t-q}$, по своим свойствам соответствующей 
<<белому шуму>>. Модель скользящего среднего порядка $q$ 
может быть выражена следующим уравнением \cite{runova2013}:
\begin{equation}
	y_t = \varepsilon_t - \gamma_1 \varepsilon_{t-1} - \gamma_2 \varepsilon_{t-2} 
		- ... - \gamma_q \varepsilon_{t-q}, 
\end{equation} 
\begin{eqwhere}
	y_t	& значение ряда в момент времени $t$ \\
	\gamma_1, \gamma_2, ..., \gamma_q 	& параметры модели, \\
	\varepsilon_t	& случайные ошибки образующие <<белый шум>>.\\
\end{eqwhere}
В эквивалентной форме:
\begin{equation}
	y_t = (1 - \gamma_1 B - \gamma_2 B^2 - ... - \gamma_q B^q)\varepsilon_t \text{,} 
\end{equation}
или:
\begin{equation}
	y_t = \gamma(B)\varepsilon_t \text{,} 
\end{equation}
где $B$ - оператор сдвига назад:
\begin{equation}
	B y_t = y_{t-1} \text{.} \nonumber
\end{equation}

То есть, процесс скользящего среднего можно трактовать как выход $y_t$ 
линейного фильтра с передаточной функцией $\gamma(B)$, на вход которого 
подается процесс белого шума $\varepsilon_t$ \cite{box2008}.


\subsection{Модель авторегрессии AR(p)}
Модель авторегрессии порядка $p$ может быть представлена в следующем 
виде \cite{runova2013}:
\begin{equation}
	y_t = \beta_1 y_{t-1} + \beta_2 y_{t-2} + ... + \beta_p y_{t-p} + \varepsilon_t \text{,} 
\end{equation}
где $\beta_1, \beta_2, ..., \beta_p$ -- параметры модели. Таким образом прогнозируемое значение представляется в виде линейной 
зависимости от $p$ известных значений временного ряда. 

В эквивалентной форме:
\begin{equation}
	y_t = \frac{\varepsilon_t}{1 - \beta_1 B - \beta_2 B^2 - ... - \beta_p B^p} \text{,} 
\end{equation}
или:
\begin{equation}
	y_t = \beta^{-1}(B)\varepsilon_t \text{,}
\end{equation}
отсюда процесс авторегрессии можно трактовать как выход $y_t$ линейного 
фильтра с передаточной функцией $\beta^{-1}(B)$, на вход которого подается 
процесс белого шума $\varepsilon_t$ \cite{box2008}. 


\subsection{Модель авторегрессии -- скользящего среднего ARMA(p, q)}
Конечный процесс авторегрессии может быть представлен как бесконечный 
процесс скользящего среднего MA($\infty$) \cite{hamilton1994}, однако с 
увеличением порядка модели её расчет значительно усложняется и потому если 
процесс действительно типа AR, то его представление в виде скользящего 
среднего не может быть экономичным. Точно так же процесс MA не может быть 
экономично представлен с помощью процесса авторегресии. Чтобы 
параметризация была более экономичной в модель могут быть включены как 
члены описывающие скользящее среднее, так и члены моделирующие 
авторегрессию.

Общий вид авторегрессии -- скользящего среднего ARMA(p, q) определяется 
следующим уравнением \cite{runova2013}:
\begin{equation}
	y_t = \beta_1 y_{t-1} + \beta_2 y_{t-2} + ... + \beta_p y_{t-p} + \varepsilon_t 
		- \gamma_1 \varepsilon_{t-1} - \gamma_2 \varepsilon_{t-2}  - ... 
		- \gamma_p \varepsilon_{t-p}, 
\end{equation}
Как легко заметить, при порядке $p$ равном нулю будет получен процесс MA, 
а при обнулении порядка $q$ - процесс AR. 

Процесс  может быть записан в эквивалентной форме:
\begin{equation}
	y_t = \frac
		{1 - \gamma_1 B - \gamma_2 B^2 - ... - \gamma_q B^q}
		{1 - \beta_1 B - \beta_2 B^2 - ... - \beta_p B^p}  \varepsilon_t \text{,} 
\end{equation}
таким образом смешанный процесс авторегрессии -- скользящего среднего 
можно интерпретировать как выход $y_t$ линейного фильтра, его передаточная 
функция есть отношение двух полиномов, на вход которого подается белый шум 
$\varepsilon_t$ \cite{box2008}. 
 
 \subsection{Модель авторегрессии -- проинтегрированного скользящего среднего 
 ARIMA(p, d, q)}
 Перечисленные выше модели используются для прогнозирования стационарных 
 процессов. Это такие процессы, свойства которых не зависят от изменения 
 начала отсчета времени. Однако, существуют временные ряды, которые  ведут 
 себя так, словно они не имеют фиксированного среднего значения и даже в 
 этом случае они проявляют однородность, и если не учитывать локальный 
 уровень или, возможно, локальный уровень и тренд, то любая часть временного 
 ряда по своему поведению подобна любой другой части. Описывающие такое 
 однородное нестационарное поведение модели можно получить, сделав 
 предположение, что некая подходящая разность процесса стационарна. 
 Модели, в которых d-я разность есть стационарный смешанный процесс 
 авторегрессии -- скользящего среднего называются процессами авторегрессии 
 -- проинтегрированного скользящего среднего ARIMA(p, d, q) \cite{box2008}. 
 Данный процесс может быть представлен уравнением вида:
 \begin{equation}
	\Delta^d y_t = \beta_1 \Delta^d y_{t-1} + \beta_2 \Delta^d y_{t-2} + ... 
		+ \beta_p \Delta^d y_{t-p} + \varepsilon_t  - \gamma_1 \varepsilon_{t-1} 
		- \gamma_2 \varepsilon_{t-2} - ... - \gamma_p \varepsilon_{t-p}, 
\end{equation}
где $\Delta^d$ --  оператор разности временного ряда порядка d, например: 
\begin{equation}
	\Delta y_t = y_t - y_{t - 1} \text{,} \nonumber
\end{equation}
для первого порядка;
\begin{equation}
	\Delta^2 y_t = \Delta y_t - \Delta y_{t - 1} = (y_t - y_{t - 1}) - (y_{t - 1} - y_{t - 2}) 
		= y_t - 2y_{t - 1} + y_{t - 2} \text{,} \nonumber
\end{equation}
для второго.

\section{Прогнозирование при помощи нейросетевых методов}
Искусственные нейронные сети (далее - нейронные сети) возникли на основе 
знаний о функционировании нервной системы живых существ. Они 
представляют собой попытку использования процессов, происходящих в 
нервных системах для выработки новых технологических решений 
\cite{osovskiy2002}. 

Основным элементом обработки информации в нейронной сети является модель 
нейрона. В его основе лежат \cite{haykin1999}:
\begin{enumerate}
	\item Набор связей (connecting link) или синапсов (synapse), при этом 
		каждый синапс имеет свой вес (weight). Например, если на вход 
		синапса $j$, который связан с нейроном $k$, поступает сигнал $x_j$, то 
		этот сигнал умножается на вес $w_{k j}$. При этом веса могут иметь
		как положительное, так и отрицательное значение.
	\item Сумматор, который складывает все входные сигналы перемноженные 
		на соответсвующие им веса.
	\item Функция активация (activation function), ограничивающая амплитуду 
		выходного сигнала нейрона. Как правило, значение на выходе нейрона 
		лежит в интервале $[0, 1]$ или $[-1, 1]$. Обычно выделяют 3 основных 
		типа активационных функций: единичного скачка, кусочко-линейные и 
		сигмоидальные. Наибольшее распространение получили последние.
\end{enumerate}

Одним из важнейших преимуществ нейронных сетей является возможность 
обучения. Обучение можно рассмотреть как корректировку весов сети, 
выполняемую по обучающим примерам (или обучающим данным), в следствие 
которой сеть меняет свою реакцию на входные воздействия.

Следует отметить, что нейронные сети позволяют получить результат на ранее 
не виденных примерах данных. Поэтому нейросетевые методы хорошо себя 
зарекомендовали как средство моделирования динамических систем при 
неизвестной априори математической модели динамической системы. 
Существует два базовых метода наделения нейронных сетей свойствами, 
необходимых для прогнозирования поведения динамических систем: 
добавление линий задержек и добавление рекуррентных связей. 
\cite{chernodub2012}. Оба метода представлены в разделе далее.

\subsection{Многослойный персептрон с линией задержек}
В многослойном персептроне (Multilayer Perceptron, MLP) с линией задержек все 
нейроны расположены слоями, при этом имеется один входной слой, один 
выходной и как минимум один скрытый слой. Пример схемы данной сети 
порядка N и одним скрытым слоем изображен на рисунке \ref{fig:mlparch}. Сеть 
содержит нейроны с линейной функцией активации во входном слое и нейроны 
с сигмоидальной функцией активации в скрытом и выходном слоях. Весовые 
коэффициенты задаются матрицами $W^{(1)}$ и $W^{(2)}$. На вход нейронная 
сеть получает текущее значение временного ряда $y(k)$, а так же задержанные 
значения $y(k-1)$, $y(k-2),$ ... $y(k-N)$, полученные при помощи элементов 
запаздывания $Z^{-1}$, $Z^{-2}$, ... $Z^{-N}$. По полученным данным сеть 
обучается делать прогноз следующего значения временного ряда $y(k+1)$.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{\detokenize{mlp_base_architecture}}
	\caption{Схема многослойного персептрона с линией задержек порядка N.}
	\label{fig:mlparch}
\end{figure}  % I need to draw my own scheme

\subsection{Рекуррентный многослойный персептрон}
Рекуррентный многослойный персептрон (Recurrent Multilayer Perceptron, RMLP) 
отличается от многослойного персептрона тем, что его выходные значения 
зависят не только от значений на входе в данный момент времени, но и от 
предыдущих входных значений или состояния сети. Поэтому данный вид 
нейронных сетей получил широкое распространение в управляющих 
приложениях и приложениях направленных на обработку сигналов 
\cite{medsker2000}. Схема персептрона, используемого для прогнозирования 
временного ряда изображена на рисунке \ref{fig:rmlparch}. Как можно заметить, в 
матрице весов скрытого слоя $W^{(1)}$ теперь хранятся еще и веса 
рекуррентных связей.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{\detokenize{rmlp_base_architecture}}
	\caption{Схема рекуррентного многослойного персептрона.}
	\label{fig:rmlparch}
\end{figure}  % I need to draw my own scheme

\section{Подготовка данных для сравнения}
Для построения и тестирования моделей прогнозирования были использованы 
статистические данные, полученные с разрабатываемого сервиса-агрегатора. 
Так как сервис разрабатывается на Java, сбор информации по нагрузке 
процессора осуществлялся при помощи компонента Java платформы 
com.sun.management. OperatingSystemMXBean. Данный компонент позволяет 
получить информацию по нагрузке процессора от операционной системы 
\cite{osmxbean}. 

Каждые 300 мс производился сбор шестнадцати статистических 
характеристик, которые возможно разделить на четыре типа:
\begin{enumerate}
	\item {\bfseries Количество пришедших запросов с момента предыдущего 
														замера.\\}
		У разрабатываемого сервиса насчитывается шесть видов запросов 
		пользователей, для каждого выделено по одному отдельному  
		счетчику.
	\item {\bfseries Количество загрузчиков внешних ресурсов.\\}
		При поисковом запросе производится запуск процесса загрузки для 
		необходимых внешних ресурсов. По одной количественной 
		характеристике выделено для каждого внешнего ресурса, всего две.
	\item {\bfseries Среднее время обработки запроса с предыдущего замера по 
										текущий момент времени.\\}
		Аналогично первому типу -- шесть видов запросов.
	\item {\bfseries Нагрузка процессора.\\}
		Две отдельные характеристики: нагрузка процессом и общая нагрузка 
		процессора.
\end{enumerate}	

Графики со статистическими данными изображены на рисунке \ref{fig:sdcharts}.

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{\detokenize{statistic_data_charts}}
	\caption{Статистические данные сервиса-агрегатора.}
	\label{fig:sdcharts}
\end{figure} 

\section{Сравнение моделей прогнозирования ARIMA}
Ранее в главе был дан краткий обзор моделей прогнозирования ARIMA, и как 
можно отметить, один из основных недостатков данных моделей -- это 
необходимость повторного расчета коэффициентов при получении новых 
данных, что может негативно сказаться на производительности системы (так 
как значимая доля её процессорного времени будет уделяться данной 
проблеме). Поэтому порядок моделей и количество обучающих данных были 
ограничены. 

Для расчета ARIMA-моделей была использована Java-библиотека с открытым 
исходным кодом -- Workday/timeseries-forecast. На одних и тех же входных 
данных по нагрузке процессора, собранных за 50 минут работы системы, были 
построены модели с порядками $p \le 11$, $d \le 3$ и $q \le 11$. Количество 
обучающих данных n варьировалось от 10 до 4000. 

Оценка точности модели производилась по следующему алгоритму: 
\begin{itemize}
	\item рассчитывалась ARIMA модель с порядками $p$, $d$ и $q$ на 
		последовательных значениях нагрузки с $i$-го по $i + n - 1$ 
	\item рассчитывался прогноз $f$ следующего шага -- $i + n$
	\item происходила нормализация спрогнозированного значения по
		следующему правилу: 
				если $f < 0$, то $f_n = 0$; если $f > 1$, то $f_n = 1$
	\item фиксировалась ошибка -- разница между фактическим значением 
		временного ряда и нормализованным значением $f_n$
	\item выполненные действия повторялись при $i = i + 1$
\end{itemize}
После завершения работы данного алгоритма вычислялась средняя 
квадратическая ошибка (root mean square error RMSE, данный способ оценки 
точности часто используется для сравнения моделей пронозирования 
\cite{liu1994, mills2011}) модели порядков $p$, $d$, $q$ с количеством обучающих 
данных равным $n$. Результаты, отсортированные в порядке возрастания 
ошибки, представлены в таблицах 
\labelcref{tab:mamodels,tab:armodels,tab:armamodels,tab:arimamodels}. 
Результаты прогнозов наиболее точных моделей AR, MA, ARMA и ARIMA 
изображены на рисунке \ref{fig:arimaforecasting}.

\begin{gosttable}
	\caption{Сравнение моделей MA.}
	\begin{tabular}{| >{\centering}m{3.5cm} | >{\centering}m{3cm} | 
									>{\centering\arraybackslash}m{3.5cm} |}
		\hline
		Порядок модели MA ($q$) 
			& Количество обучающих данных ($n$)
			& Cредняя квадратическая ошибка \\ 
		\hline
		1 & 15 & 0.116628 \\ \hline
		1 & 16 & 0.116784 \\ \hline
		1 & 11 & 0.116977 \\ \hline
		1 & 13 & 0.117005 \\ \hline
		1 & 18 & 0.117108 \\ \hline
		1 & 20 & 0.117462 \\ \hline
		1 & 19 & 0.117549 \\ \hline
		1 & 14 & 0.117555 \\ \hline
		1 & 17 & 0.117649 \\ \hline
		1 & 22 & 0.117656 \\ \hline
	\end{tabular}
	\label{tab:mamodels}
\end{gosttable}

\begin{gosttable}
	\caption{Сравнение моделей AR.}
	\begin{tabular}{| >{\centering}m{3.5cm} | >{\centering}m{3cm} | 
									>{\centering\arraybackslash}m{3.5cm} |}
		\hline
		Порядок модели AR ($p$) 
			& Количество обучающих данных ($n$)
			& Cредняя квадратическая ошибка \\ 
		\hline
		9 & 800 & 0.103868 \\ \hline
		8 & 800 & 0.103885 \\ \hline
		8 & 700 & 0.103948 \\ \hline
		9 & 700 & 0.103956 \\ \hline
		10 & 800 & 0.103992 \\ \hline
		10 & 700 & 0.104136 \\ \hline
		7 & 800 & 0.104277 \\ \hline
		9 & 900 & 0.104283 \\ \hline
		8 & 900 & 0.104333 \\ \hline
		7 & 700 & 0.104349 \\ \hline
	\end{tabular}
	\label{tab:armodels}
\end{gosttable}

\begin{gosttable}
	\caption{Сравнение моделей ARMA.}
	\begin{tabular}{| >{\centering}m{3cm} | >{\centering}m{3cm} | 
									>{\centering\arraybackslash}m{3.5cm} |}
		\hline
		Порядки модели ARMA ($p, q$) 
			& Количество обучающих данных ($n$)
			& Cредняя квадратическая ошибка \\ 
		\hline
		8, 1 & 800 & 0.104518 \\ \hline
		9, 1 & 800 & 0.104555 \\ \hline
		10, 1 & 700 & 0.104639 \\ \hline
		9, 1 & 700 & 0.104718 \\ \hline
		8, 1 & 700 & 0.104766 \\ \hline
		10, 1 & 900 & 0.104776 \\ \hline
		10, 1 & 800 & 0.104785 \\ \hline
		9, 2 & 700 & 0.104795 \\ \hline
		9, 3 & 800 & 0.104845 \\ \hline
		9, 1 & 900 & 0.104894 \\ \hline
	\end{tabular}
	\label{tab:armamodels}
\end{gosttable}

\begin{gosttable}
	\caption{Сравнение моделей ARIMA.}
	\begin{tabular}{| >{\centering}m{4cm} | >{\centering}m{3cm} | 
									>{\centering\arraybackslash}m{3.5cm} |}
		\hline
		Порядки модели ARIMA ($p, d, q$) 
			& Количество обучающих данных ($n$)
			& Cредняя квадратическая ошибка \\ 
		\hline
		0, 1, 3 & 700 & 0.103599 \\ \hline
		7, 1, 0 & 700 & 0.103604 \\ \hline
		8, 1, 0 & 700 & 0.103613 \\ \hline
		8, 1, 0 & 800 & 0.103665 \\ \hline
		7, 1, 0 & 800 & 0.103714 \\ \hline
		7, 1, 0 & 600 & 0.103752 \\ \hline
		0, 1, 3 & 800 & 0.103774 \\ \hline
		8, 1, 0 & 600 & 0.103777 \\ \hline
		10, 1, 0 & 800 & 0.103798 \\ \hline
		9, 1, 0 & 700 & 0.103804 \\ \hline
	\end{tabular}
	\label{tab:arimamodels}
\end{gosttable}
\vspace{1em}
\begin{figure}[!h]
	\centering
	\includegraphics[width=\linewidth]{\detokenize{arima_forecasting}}
	\caption{Прогноз нагрузки процессора.}
	\label{fig:arimaforecasting}
\end{figure} 

\newpage
На рисунке \ref{fig:arimaforecasting} можно отметить, что резкие скачки нагрузки 
не прогнозируются ни одной из моделей. Кроме того наиболее точные модели 
вычисляют повторение исходного ряда, но смещенного по времени. По сути 
значения рассчитанные любой из вышеперечисленных моделей нельзя считать 
прогнозом.

\section{Сравнение нейросетевых моделей} %just sketch
Ранее в главе были представлены два типа нейронных сетей, используемых для прогнозирования временных рядов. Следует отметить, что основными их отличиями являются:
\begin{enumerate}
	\item При использовании многослойного персептрона, в отличие от 
		рекуррентного, необходимо заранее знать порядок линии задержек на 
		входе (в пределах данной работы это не является проблемой, так как 
		порядок может быть выбран в ходе экспериментов, и больше не 
		меняться в ходе работы системы).
	\item Рекуррентные сети имеют большую точность при многошаговом 
		прогнозировании \cite{chernodub2012}, однако это не имеет значения в 
		данной работе.
	\item Обучение рекуррентных сетей является более трудоемкой задачей, в 
		следствие наличия у них дополнительных степеней свободы \cite{naseera2015, chernodub2012}.
\end{enumerate}

На вход нейронной сети подается шестнадцать динамических характеристик (а 
также $n - 1$ предыдущих значений каждой из характеристик для сети с линией 
задержек порядка $n$). На выходе ожидается бинарное значение: 1 -- до 
следующего замера данные должны браться из кэша; 0 -- данные выгружаются 
непосредственно с внешних ресурсов. Так как при слишком высокой нагрузке 
повышается среднее время обработки запросов, то в качестве прогнозируемого 
значения возьмем факт превышения данной характеристикой у любого из шести 
видов запросов какого-то порогового значения (в пределах данной работы -- 90 
мс.).

Для составления и обучения нейронной сети был использован Java"=фреймворк 
с открытым исходным кодом -- Encog. В качестве обучающего был взят тот же 
участок данных, что использовался при построении моделей ARIMA в 
предыдущем разделе.

\section{Выводы по результатам сравнительного анализа}
\begin{enumerate}
	\item Модели AR, MA, ARMA, ARIMA не подходят для прогнозирования 
		нагрузки процессора, в силу недостаточной точности на данном виде 
		временного ряда.
	\item Модель прогнозирования, составленная на базе многослойного 
		пересептрона, имеет небольшой процент ошибки на обучающей 
		выборке и будет использоваться при разработке алгоритма 
		распределения нагрузки.
\end{enumerate}

\likechapter{Заключение}
\printbibliography[heading=bibheading]
 
\end{document}
