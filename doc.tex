\documentclass[a4paper,14pt,russian]{extreport}

\usepackage{extsizes}
\usepackage{cmap}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{pscyr}
\usepackage{indentfirst}
\renewcommand{\rmdefault}{ftm}
\frenchspacing
\usepackage[nodisplayskipstretch]{setspace}
\onehalfspacing

\usepackage{graphicx}
\graphicspath{ {images/} } 

\usepackage{amssymb,amsfonts,amsmath,amsthm}
\usepackage[usenames,dvipsnames]{color}
\usepackage{makecell}
\usepackage{multirow}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{syntax}
\usepackage{listings}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[C]{\thepage}
\fancyheadoffset{0mm}
\fancyfootoffset{0mm}
\setlength{\headheight}{17pt}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\fancypagestyle{plain}{ 
	\fancyhf{}
	\chead{\thepage}}
\setcounter{page}{4}

\renewcommand{\arraystretch}{1.4}
\setlength{\floatsep}{2em}

\usepackage[tableposition=top]{caption}
\usepackage{subcaption}
\DeclareCaptionLabelFormat{gostfigurelabel}{Рисунок #2}
\DeclareCaptionLabelFormat{gosttablelabel}{Таблица #2}
\DeclareCaptionLabelSeparator{gost}{~---~}
\captionsetup{labelsep=gost}
\captionsetup[figure]{labelformat=gostfigurelabel}
\captionsetup[table]
	{labelformat=gosttablelabel, justification=raggedright, singlelinecheck = false}
\usepackage{threeparttable}
\renewcommand{\thesubfigure}{\asbuk{subfigure}}

\usepackage{cleveref}

\usepackage{geometry}
\geometry{left=2.5cm}
\geometry{right=1.0cm}
\geometry{top=2.0cm}
\geometry{bottom=2.0cm}

	\usepackage{titlesec}
	 
	\titleformat{\chapter}
		{\filcenter\bfseries}
		{\thechapter}
		{8pt}
		{\MakeUppercase}
	 
	\titleformat{\section}
		{\normalsize\bfseries}
		{\thesection}
		{1em}{}
	 
	\titleformat{\subsection}
		{\normalsize\bfseries}
		{\thesubsection}
		{1em}{}
	
	\titlespacing*{\chapter}{0pt}{-30pt}{8pt}
	\titlespacing*{\section}{\parindent}{*4}{*4}
	\titlespacing*{\subsection}{\parindent}{*4}{*4}
	
	\usepackage{enumitem}
	\makeatletter
	    \AddEnumerateCounter{\asbuk}{\@asbuk}{м)}
	\makeatother
	\renewcommand{\labelitemi}{-}
	\renewcommand{\labelenumi}{\arabic{enumi}.}
	\renewcommand{\labelenumii}{\asbuk{enumii}.}
	
	\usepackage{tocloft}
	\renewcommand{\cfttoctitlefont}{\hspace{0.38\textwidth} \bfseries\MakeUppercase}
	\renewcommand{\cftaftertoctitleskip}{2em}
	\renewcommand{\cftbeforetoctitleskip}{-1em}
	\renewcommand{\cftchapfont}{\normalsize\bfseries}
	\renewcommand{\cftdotsep}{1}
	\setcounter{tocdepth}{2}
	
	\setlength\cftbeforechapskip{1em}
	\setlength\cftbeforesecskip{.5em}
	\setlength\cftbeforesubsecskip{.25em}
	
	\newcommand{\empline}{\vspace{1em}}
	\newcommand{\likechapterheading}[1]{ 
		\clearpage   
		\begin{center}
		\textbf{\MakeUppercase{#1}}
		\end{center}
	}

\usepackage{etoolbox}
	\makeatletter
		\patchcmd{\l@chapter}{#1}{\MakeUppercase{#1}}{}{}
		\renewcommand{\@dotsep}{1}
		\newcommand{\l@likechapter}[2]
			{{\bfseries\@dottedtocline{0}{0pt}{0pt}{#1}{\bfseries#2}}}
	\makeatother
	\newcommand{\likechapter}[1]{ 
		\likechapterheading{#1}    
		\addtocontents{toc}{\vspace{1em}}
		\addcontentsline{toc}{likechapter}{\MakeUppercase{#1}}
		\empline
	}
	
	\usepackage[
		backend=biber,
		bibencoding=utf8,
		sorting=none,
		style=gost-numeric,
		autolang=none,
		eprint=false
	]{biblatex}
	\addbibresource{references/literature.bib}

\defbibheading{bibheading}[Библиографический список]{
	\likechapterheading{#1}
	\addtocontents{toc}{\vspace{1em}}
	\addcontentsline{toc}{likechapter}{\MakeUppercase{#1}}
}
\DeclareFieldFormat{author}{{#1}}

\usepackage{lastpage}

	\usepackage[title,titletoc]{appendix}
	 
	\titleformat{\paragraph}[display]
		{\filcenter}
		{\MakeUppercase{\chaptertitlename} \thechapter}
		{8pt}
		{\bfseries}{}
	\titlespacing*{\paragraph}{0pt}{-30pt}{8pt}
	 
	\newcommand{\append}[1]{  
		\clearpage
		\stepcounter{chapter}    
		\paragraph{\MakeUppercase{#1}}
		\empline
		\addcontentsline{toc}{likechapter}
					{\MakeUppercase{\chaptertitlename~\Asbuk{chapter}\;#1}}}

\setlength{\parindent}{1.25cm} 
\setlength{\parskip}{6pt}


\newenvironment{gosttable}
	{
		\begin{table}[!h]
			\centering
			\begin{threeparttable}
	}	
	{
			\end{threeparttable}
		\end{table}
	}
	
\newenvironment{eqwhere}
	{где:
		\par\noindent\hspace{2em}\begin{tabular}{>{$}r<{$} @{${}$ -- {}} l}
	}
	{\end{tabular}\par\vspace{\belowdisplayskip}}

\begin{document}

\tableofcontents

\likechapter{Введение}

{\bfseries Актуальность темы исследования.} 
В последнее время большое распространение получили различные 
сервисы-агрегаторы, объединяющие данные из нескольких источников
определённой тематики в один, что уменьшает затрачиваемое пользователем
время на поиск необходимой информации. 

Для обработки запроса от одного пользователя, агрегатору может 
потребоваться совершить десятки или даже сотни запросов к сторонним 
ресурсам. В силу того, что пользователи могут запрашивать одну и ту же 
информацию, самым распространённым способ снижения времени ответа 
является использование промежуточных буферов, так называемых кэшей. Но 
данный способ приводит к снижению актуальности предоставляемых 
агрегатором данных и может быть использован для предупреждения 
избыточной нагрузки системы, либо для хранения редко обновляемых данных, 
таких как словари. Словарь -- это набор статических данных используемых 
агрегатором для объединения информации получаемой с нескольких источников 
(например, для агрегатора отелей словарём будет являться список всех отелей 
по которым осуществляется поиск). 

Таким образом, разработка алгоритма распределения нестационарной нагрузки, 
определяющего источник данных агрегатора, является актуальной задачей. 
Однако для её решения необходим анализ или прогноз нагрузки системы.

{\bfseries Степень теоретической разработанности темы.}
В открытом доступе существует множество научных работ, описывающих 
методы прогнозирования временных рядов. Но материалы, описывающие 
проблему выбора метода прогнозирования для разработки алгоритма 
распределения нестационарной нагрузки, найти не удалось. Из сказанного выше 
можно сделать вывод о том, что рассматриваемая тема имеет низкую степень 
теоретической проработанности.

{\bfseries Объектом исследования} является прогнозирование нестационарной 
нагрузки сервиса"=агрегатора.

{\bfseries Предметом исследования} являются методы прогнозирования 
временных рядов.

{\bfseries Область исследования.} Проведённое исследование методов 
прогнозирования нагрузки вычислительной системы в пределах разработки 
алгоритма распределения запросов пользователей полностью соответствует 
специальности «Вычислительные машины, комплексы, системы и сети», а 
содержание выпускной квалификационной работы -- техническому
заданию.

{\bfseries Цель и задачи исследования.}  Целью работы является улучшение 
качества обслуживания пользователей сервиса-агрегатора за счет снижения 
среднего времени ответа.

Для достижения данной цели были поставлены следующие задачи:
\begin{enumerate}
	\item Выполнить сравнительный анализ методов прогнозирования.
	\item Разработать алгоритм распределения нестационарной нагрузки и его 
		программную реализацию на основе выбранного метода 
		прогнозирования.
	\item Выполнить сравнение времени ответа исходной и использующей 
		разработанный алгоритм систем.
\end{enumerate}

{\bfseries Теоретическую основу исследования} составляют научные труды
отечественных и зарубежных авторов в области компьютерных технологий и
математической статистики.

{\bfseries Методологическую основу исследования} составляет эксперимент.

{\bfseries Научная новизна работы} заключается в следующем:
\begin{enumerate}
	\item В настоящее время не существует структурированных рекомендаций 
		по выбору метода прогнозирования при разработке алгоритмов 
		распределения нагрузки, представленных в данной работе. 
	\item Разработанный алгоритм распределения нестационарной нагрузки 
		может обеспечить улучшение качества обслуживания пользователей
		сервиса"=агрегатора.
\end{enumerate}

{\bfseries Практическая значимость} данной работы заключается в том, что 
разработанный алгоритм распределения нестационарной нагрузки будет
интегрирован в разрабатываемый сервис"=агрегатор. Кроме того, данный 
алгоритм может быть интегрирован существующими сторонними сервисами, а 
сформулированные рекомендации могут быть использованы для осуществления 
выбора метода прогнозирования при разработке собственного алгоритма 
распределения запросов пользователей.

{\bfseries Апробация результатов исследования.} Сформулированные 
рекомендации по выбору метода прогнозирования обсуждались на VII Конгрессе 
молодых учёных, а их сравнительный анализ был представлен на XLVII научной 
и учебно-методической конференции.

{\bfseries Объем и структура работы.} 


\chapter{Сравнительный анализ методов прогнозирования}
Необходимость в точном прогнозе возникает во многих областях науки, 
индустрии, коммерческой и экономической деятельностях. Ситуации его 
использования весьма разнообразны, и в зависимости от исходной проблемы, 
прогнозирование может осуществляться вперед на несколько лет, а может на 
несколько секунд. 

Прогнозирование является важной составляющей эффективного планирования 
ресурсов. В качестве примера может быть приведено прогнозирование 
электрических нагрузок в электроэнергетике: на основе предсказанных величин 
рассчитываются оптимальные режимы электро"=энергетических систем, а 
точность прогноза влияет на экономичность загрузки генерирующего 
оборудования, и, следовательно, на стоимость электроэнергии.

Метод прогнозирования - это процедура вычисления прогноза из текущих и 
прошлых значений. Это может быть лишь алгоритмическое правило не 
зависящее от базовой вероятностной модели, поведение которой необходимо 
предсказать. В другом случае, метод может использоваться с целью 
определения конкретной модели для предоставленных данных или поиска 
оптимальных условий прогнозирования для такой модели. Поэтому стоит 
различать понятия <<модель>> и <<метод>> \cite{chatfield2000}. 

Существует широкий спектр методов прогнозирования, по оценкам зарубеж-
ных и отечественных систематиков прогностики их насчитывается
уже свыше ста \cite{tihonov2006}. Каждый имеет собственные характеристики 
(такие как точность или вычислительная сложность), которые должны быть 
учтены при выборе метода.

Методы прогнозирования можно разделить на три основных типа 
\cite{chatfield2000, armstrong1999, brockwell2002, hyndman2012}:
\begin{enumerate}
	\item {\itshape Методы суждений} основанные на субъективных суждениях, 
		интуиции, <<внутренних>> коммерческих знаниях, или любой другой 
		подобной информации.
	\item {\itshape Одномерные методы} где предсказание зависит только от 
		прошлых и текущего значений предсказываемого ряда.
	\item {\itshape Многомерные методы} в которых прогнозирование величины, 
		по крайней мере частично, так же зависит от одного или б\'{о}льшего 
		количества временных рядов. Такую совокупность, состоящую из 
		нескольких рядов, называют многомерными временными рядами 
		\cite{popov2006}.
\end{enumerate}

Все рассматриваемые в данной главе методы осуществляли прогнозирование 
поведения временных рядов. Временной ряд - это ряд наблюдений, 
произведенных последовательно во времени \cite{chatfield2000, armstrong1999, 
brockwell2002, box2008, wolters2007, hyndman2012}. Он включает два 
обязательных элемента: время и конкретное значение показателя, или уровень 
ряда \cite{popov2006, afanasyev2001}. 

Временным рядом можно назвать огромное количество последовательностей, 
являющихся следствием измерений некоторого показателя. Например, 
показатели (характеристики) экономических, природных, промышленных, 
информационных и других систем. Фактически, временной ряд может быть 
построен тремя способами: путем отбора значений из непрерывного ряда, путем 
агрегирования данных за установленные периоды времени, и взятием серии 
дискретных наблюдений \cite{chatfield2000, box2008}. Для всех трех типов запись 
данных производится с равными промежутками времени.

Временные ряды подразделяются на стационарные и нестационарные. Ряд 
называется стационарным, если его среднее значение, дисперсия и ковариация 
не зависят от времени \cite{popov2006}. Другими словами, это ряд параметры 
которого не зависят от того, в какой момент времени производились замеры 
\cite{hyndman2012}. Ряд не соответствующий данным условиям называется 
нестационарным.

Одним из наиболее важных видов временных рядов является 
последовательность некоррелированных (не взаимосвязанных) случайных 
величин с нулевым математическим ожиданием и постоянной дисперсией 
\cite{fuller1996, runova2013}, такой ряд называется белым шумом.

В данной главе производится сравнительный анализ наиболее популярных 
одномерных и многомерных методов прогнозирования. Методы суждений не 
рассматриваются в виду того, что их невозможно автоматизировать. Так как 
выбранный в результате сравнительного анализа метод будет использоваться 
для предупреждения избыточной нагрузки, то ко всем методам предъявлялось 
требование: ошибка предсказания пиков избыточной нагрузки (то есть 
перегруженного состояния системы, при отсутствие перегрузки на момент 
расчета прогноза) должна составлять не более 5 \%. 

\section{Модели прогнозирования временных рядов AR, MA, ARMA, ARIMA} 
Среди моделей прогнозирования наибольшее распространение получили 
модели ARIMA (autoregressive integrated moving average: интегрированная модель 
авторегрессии -- скользящего среднего, или модель Бокса -- Дженкинса). Класс 
моделей ARIMA - это важный инструмент прогнозирования и базис многих 
фундаментальных идей в анализе временных рядов \cite{chatfield2000}. Он 
удобен тем, что сочетает в себе модели ARMA, AR, MA и может использоваться 
для прогнозирования как стационарных так и нестационарных временных рядов. 
Более того, данным классом учитывается связь между случайными остатками 
временного ряда, которые можно получить вычтя из исходного ряда его 
неслучайную составляющую (тренд) \cite{tihonov2006}. Далее в разделе 
представлено описание моделей.


\subsection{Модель скользящего среднего MA(q)}
В моделях скользящего среднего текущее значение ряда представляется в виде 
линейной комбинации текущего и прошедших значений ошибки $\varepsilon_t$, 
$\varepsilon_{t-1}$, $...$, $\varepsilon_{t-q}$, по своим свойствам соответствующей 
<<белому шуму>>. Модель скользящего среднего порядка $q$ 
может быть выражена следующим уравнением \cite{runova2013}:
\begin{equation}
	y_t = \varepsilon_t - \gamma_1 \varepsilon_{t-1} - \gamma_2 \varepsilon_{t-2} 
		- ... - \gamma_q \varepsilon_{t-q}, 
\end{equation} 
\begin{eqwhere}
	y_t	& значение ряда в момент времени $t$ \\
	\gamma_1, \gamma_2, ..., \gamma_q 	& параметры модели, \\
	\varepsilon_t	& случайные ошибки образующие <<белый шум>>.\\
\end{eqwhere}
В эквивалентной форме:
\begin{equation}
	y_t = (1 - \gamma_1 B - \gamma_2 B^2 - ... - \gamma_q B^q)\varepsilon_t \text{,} 
\end{equation}
или:
\begin{equation}
	y_t = \gamma(B)\varepsilon_t \text{,} 
\end{equation}
где $B$ - оператор сдвига назад:
\begin{equation}
	B y_t = y_{t-1} \text{.} \nonumber
\end{equation}

То есть, процесс скользящего среднего можно трактовать как выход $y_t$ 
линейного фильтра с передаточной функцией $\gamma(B)$, на вход которого 
подается процесс белого шума $\varepsilon_t$ \cite{box2008}.


\subsection{Модель авторегрессии AR(p)}
Модель авторегрессии порядка $p$ может быть представлена в следующем 
виде \cite{runova2013}:
\begin{equation}
	y_t = \beta_1 y_{t-1} + \beta_2 y_{t-2} + ... + \beta_p y_{t-p} + \varepsilon_t \text{,} 
\end{equation}
где $\beta_1, \beta_2, ..., \beta_p$ -- параметры модели. Таким образом 
прогнозируемое значение представляется в виде линейной зависимости от $p$ 
известных значений временного ряда. 

В эквивалентной форме:
\begin{equation}
	y_t = \frac{\varepsilon_t}{1 - \beta_1 B - \beta_2 B^2 - ... - \beta_p B^p} \text{,} 
\end{equation}
или:
\begin{equation}
	y_t = \beta^{-1}(B)\varepsilon_t \text{,}
\end{equation}
отсюда процесс авторегрессии можно трактовать как выход $y_t$ линейного 
фильтра с передаточной функцией $\beta^{-1}(B)$, на вход которого подается 
процесс белого шума $\varepsilon_t$ \cite{box2008}. 


\subsection{Модель авторегрессии -- скользящего среднего ARMA(p, q)}
Конечный процесс авторегрессии может быть представлен как бесконечный 
процесс скользящего среднего MA($\infty$) \cite{hamilton1994, chatfield2000}, 
однако с увеличением порядка модели её расчет значительно усложняется и 
потому если процесс действительно типа AR, то его представление в виде 
скользящего среднего не может быть экономичным. Точно так же процесс MA не 
может быть экономично представлен с помощью процесса авторегресии. Чтобы 
параметризация была более экономичной в модель могут быть включены как 
члены описывающие скользящее среднее, так и члены моделирующие 
авторегрессию.

Общий вид авторегрессии -- скользящего среднего ARMA(p, q) определяется 
следующим уравнением \cite{runova2013}:
\begin{equation}
	y_t = \beta_1 y_{t-1} + \beta_2 y_{t-2} + ... + \beta_p y_{t-p} + \varepsilon_t 
		- \gamma_1 \varepsilon_{t-1} - \gamma_2 \varepsilon_{t-2}  - ... 
		- \gamma_p \varepsilon_{t-p}, 
\end{equation}
Как легко заметить, при порядке $p$ равном нулю будет получен процесс MA, 
а при обнулении порядка $q$ - процесс AR. 

Процесс  может быть записан в эквивалентной форме:
\begin{equation}
	y_t = \frac
		{1 - \gamma_1 B - \gamma_2 B^2 - ... - \gamma_q B^q}
		{1 - \beta_1 B - \beta_2 B^2 - ... - \beta_p B^p}  \varepsilon_t \text{,} 
\end{equation}
таким образом смешанный процесс авторегрессии -- скользящего среднего 
можно интерпретировать как выход $y_t$ линейного фильтра, его передаточная 
функция есть отношение двух полиномов, на вход которого подается белый шум 
$\varepsilon_t$ \cite{box2008}. 
 
 \subsection{Модель авторегрессии -- проинтегрированного скользящего среднего 
 ARIMA(p, d, q)}
 Перечисленные выше модели используются для прогнозирования стационарных 
 процессов. Однако, существуют временные ряды, которые  ведут 
 себя так, словно они не имеют фиксированного среднего значения и даже в 
 этом случае они проявляют однородность, и если не учитывать локальный 
 уровень или, возможно, локальный уровень и тренд, то любая часть временного 
 ряда по своему поведению подобна любой другой части. Описывающие такое 
 однородное нестационарное поведение модели можно получить, сделав 
 предположение, что некая подходящая разность процесса стационарна. 
 Модели, в которых d-я разность есть стационарный смешанный процесс 
 авторегрессии -- скользящего среднего называются процессами авторегрессии 
 -- проинтегрированного скользящего среднего ARIMA(p, d, q) \cite{box2008}. 
 Данный процесс может быть представлен уравнением вида:
 \begin{equation}
	\Delta^d y_t = \beta_1 \Delta^d y_{t-1} + \beta_2 \Delta^d y_{t-2} + ... 
		+ \beta_p \Delta^d y_{t-p} + \varepsilon_t  - \gamma_1 \varepsilon_{t-1} 
		- \gamma_2 \varepsilon_{t-2} - ... - \gamma_p \varepsilon_{t-p}, 
\end{equation}
где $\Delta^d$ --  оператор разности временного ряда порядка d, например: 
\begin{equation}
	\Delta y_t = y_t - y_{t - 1} \text{,} \nonumber
\end{equation}
для первого порядка;
\begin{equation}
	\Delta^2 y_t = \Delta y_t - \Delta y_{t - 1} = (y_t - y_{t - 1}) - (y_{t - 1} - y_{t - 2}) 
		= y_t - 2y_{t - 1} + y_{t - 2} \text{,} \nonumber
\end{equation}
для второго.

\section{Прогнозирование при помощи нейросетевых методов}
Искусственные нейронные сети (далее - нейронные сети) возникли на основе 
знаний о функционировании нервной системы живых существ. Они 
представляют собой попытку использования процессов, происходящих в 
нервных системах для выработки новых технологических решений 
\cite{osovskiy2002}. 

Основным элементом обработки информации в нейронной сети является модель 
нейрона. В его основе лежат \cite{haykin1999}:
\begin{enumerate}
	\item Набор связей (connecting link) или синапсов (synapse), при этом 
		каждый синапс имеет свой вес (weight). Например, если на вход 
		синапса $j$, который связан с нейроном $k$, поступает сигнал $x_j$, то 
		этот сигнал умножается на вес $w_{k j}$. При этом веса могут иметь
		как положительное, так и отрицательное значение.
	\item Сумматор, который складывает все входные сигналы перемноженные 
		на соответсвующие им веса.
	\item Функция активация (activation function), ограничивающая амплитуду 
		выходного сигнала нейрона. Как правило, значение на выходе нейрона 
		лежит в интервале $[0, 1]$ или $[-1, 1]$. Обычно выделяют 3 основных 
		типа активационных функций: единичного скачка, кусочко-линейные и 
		сигмоидальные. Наибольшее распространение получили последние.
\end{enumerate}

Одним из важнейших преимуществ нейронных сетей является возможность 
обучения. Обучение можно рассмотреть как корректировку весов сети, 
выполняемую по обучающим примерам (или обучающим данным), в следствие 
которой сеть меняет свою реакцию на входные воздействия.

Следует отметить, что нейронные сети позволяют получить результат на ранее 
не виденных примерах данных. Поэтому нейросетевые методы хорошо себя 
зарекомендовали как средство моделирования динамических систем при 
неизвестной априори математической модели динамической системы. 
Существует два базовых метода наделения нейронных сетей свойствами, 
необходимых для прогнозирования поведения динамических систем: 
добавление линий задержек и добавление рекуррентных связей
\cite{chernodub2012}. Оба метода могут использоваться как одномерные или 
двумерные и потому представляют особый интерес в данной работе. Их 
описание представлено в разделе далее.

\subsection{Многослойный персептрон с линией задержек}
В многослойном персептроне (Multilayer Perceptron, MLP) с линией задержек все 
нейроны расположены слоями, при этом имеется один входной слой, один 
выходной и как минимум один скрытый слой. Пример схемы данной сети 
порядка N и одним скрытым слоем изображен на рисунке \ref{fig:mlparch}. Сеть 
содержит нейроны с линейной функцией активации во входном слое и нейроны 
с сигмоидальной функцией активации в скрытом и выходном слоях. Весовые 
коэффициенты задаются матрицами $W^{(1)}$ и $W^{(2)}$. На вход нейронная 
сеть получает текущее значение временного ряда $y(k)$, а так же задержанные 
значения $y(k-1)$, $y(k-2),$ ... $y(k-N)$, полученные при помощи элементов 
запаздывания $Z^{-1}$, $Z^{-2}$, ... $Z^{-N}$. По полученным данным сеть 
обучается делать прогноз следующего значения временного ряда $y(k+1)$.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{\detokenize{mlp_base_architecture}}
	\caption{Схема многослойного персептрона с линией задержек порядка N.}
	\label{fig:mlparch}
\end{figure}  % I need to draw my own scheme

\subsection{Рекуррентный многослойный персептрон}
Рекуррентный многослойный персептрон (Recurrent Multilayer Perceptron, RMLP) 
отличается от многослойного персептрона тем, что его выходные значения 
зависят не только от значений на входе в данный момент времени, но и от 
предыдущих входных значений или состояния сети. Поэтому данный вид 
нейронных сетей получил широкое распространение в управляющих 
приложениях и приложениях направленных на обработку сигналов 
\cite{medsker2000}. Схема персептрона, используемого для прогнозирования 
временного ряда изображена на рисунке \ref{fig:rmlparch}. Как можно заметить, в 
матрице весов скрытого слоя $W^{(1)}$ теперь хранятся еще и веса 
рекуррентных связей.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{\detokenize{rmlp_base_architecture}}
	\caption{Схема рекуррентного многослойного персептрона.}
	\label{fig:rmlparch}
\end{figure}  % I need to draw my own scheme

\section{Подготовка данных для сравнения} \label{sec:datapreparation}
Для сбора данных был построен тестовый стенд, включающий в себя две 
вычислительные системы. Первая под управлением операционной системы  
Ubuntu 16.04.3 LTS, вторая под управлением операционной системы macOS High 
Sierra 10.13.3. Аппаратное обеспечение второй вычислительной системы:
\begin{itemize}
	\item {\itshape центральный процессор} Intel Core i5-4258U3,
	\item {\itshape оперативная память} 8 GB 1600 MHz DDR3,
	\item {\itshape внешняя память} SSD 128 GB.
\end{itemize}

На вычислительных ресурсах второй системы был развернут разрабатываемый 
сервис агрегатор, а так же система управления базами данных для его работы. 
Сервис принимал HTTP"=запросы, осуществлял необходимые для их 
выполнения действия и выдавал результаты. Кроме того, сервис в режиме 
реального времени осуществлял сбор статистических данных и сохранял их в 
файле в формате tsv (tab separated values -- значения, разделённые табуляцией). 
Схема тетового стенда изображена на рисунке \ref{fig:standbefore}. 

Так как сервис разрабатывается на Java, сбор информации по нагрузке 
процессора осуществлялся при помощи компонента Java платформы 
\url{com.sun.management.OperatingSystemMXBean}. Данный компонент позволяет 
получить информацию по нагрузке процессора от операционной системы 
\cite{osmxbean}. При помощи компонента \url{java.lang.Runtime} сервисом 
производился сбор информации об объёме памяти доступной виртуальной 
машине Java \cite{javaruntime}.

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{\detokenize{stand_before}}
	\caption{Схема тестового стенда.}
	\label{fig:standbefore}
\end{figure} 

Каждые 300 мс сервисом производился сбор семнадцати статистических 
характеристик, которые возможно разделить на пять типов:
\begin{enumerate}
	\item {\bfseries Количество пришедших запросов с момента предыдущего 
														замера.\\}
		У разрабатываемого сервиса насчитывается шесть видов запросов 
		пользователей, для каждого выделено по одному отдельному  
		счетчику.
	\item {\bfseries Количество загрузчиков внешних ресурсов.\\}
		При поисковом запросе производится запуск процесса загрузки для 
		необходимых внешних ресурсов. По одной количественной 
		характеристике выделено для каждого внешнего ресурса, всего две.
	\item {\bfseries Среднее время обработки запроса с предыдущего замера по 
										текущий момент времени.\\}
		Аналогично первому типу -- шесть видов запросов.
	\item {\bfseries Нагрузка процессора.\\}
		Две отдельные характеристики: нагрузка процессом и общая нагрузка 
		процессора.
	\item {\bfseries Объем доступной памяти.\\}
		Одна характеристика: объем памяти доступной виртуальной машине 
		Java. 
\end{enumerate}
Характеристики первого и второго типа предоставляют информацию о том, на 
что именно расходуются вычислительные ресурсы системы. По общей 
тенденции данных временных рядов может быть предсказан факт снижения или 
увеличения нагрузки системы.
Характеристики третьего и четвёртого типов широко используются в 
нагрузочном тестировании как основные показатели производительности 
системы \cite{molyneaux2015} и потому могут быть использованы при 
прогнозировании её перегрузки. 
На основании значений пятой характеристики может быть спрогнозирован 
запуск сборщика мусора виртуальной машиной Java, данное действие оказывает 
значительное влияние на производительность системы \cite{hunt2016}. 
Графики со статистическими данными изображены на рисунке \ref{fig:sdcharts}.

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{\detokenize{statistic_data_charts}}
	\caption{Статистические данные сервиса-агрегатора.}
	\label{fig:sdcharts}
\end{figure} 

Генерация HTTP-запросов осуществлялась средствами Apache JMeter. Был 
составлен тестовый план, потоки которого имитировали работу браузера 
пользователя, производя следующие действия:
\begin{enumerate}
	\item Запрос начальной информации, необходимой для отображения 
		стартовой страницы.
	\item Отправка запросов, имитирующих <<живой поиск>> по словарям 
		агрегатора. 
	\item Запрос на поиск со случайными параметрами.
	\item Отправка периодичных запросов в ожидании завершения поиска.
	\item Отправка запросов на получение результатов поиска.
\end{enumerate}
Временные интервалы между запросами устанавливались согласно правилам: 
\begin{itemize}
	\item среднее время позиционирования курсора и нажатия на клавишу 
		<<мыши>> составляет 0.4 с. \cite{korablev2011},
	\item cреднее время ввода символа при помощи клавиатуры составляет 
		0.25 с. \cite{korablev2011},
	\item при ожидании завершения поиска период отправки запросов 
		составляет 1 с.
\end{itemize}
Варьирование нагрузки производилось путем изменения числа одновременно 
исполняемых потоков. Было произведено стресс"=тестирование с целью 
определения числа потоков, приводящего систему к перегрузкам. Считалось, 
что система подвержена избыточной нагрузке, если среднее время выполнения 
любого из шести видов запросов составляло больше 90 мс. Во время 
тестирования каждые 2 часа происходило увеличение активных потоков на 10, 
затем рассчитывался процент времени перегрузки системы (например при 470 
активных потоках система находилась в состоянии перегрузки 6.31 \% времени). 
Всего на тестирование было затрачено около трёх суток. 

Был написан awk-скрипт, генерирующий расписание активных потоков для 
плагина Apache JMeter -- Ultimate Thread Groups. При генерации соблюдалось 
ограничение на число активных потоков: во время стрессового тестирования 
было установлено, что при 560 активных потоках система более 95 \% (98.14 \%) 
времени находилась в состоянии перегрузки, поэтому большее количество 
потоков не использовалось. Следует отметить, что все расписание может быть 
условно разделено на фазы трех типов: возрастание числа активных потоков, 
удержание числа потоков на постоянном уровне и уменьшение числа потоков. 
Посредством переключения фаз производится перевод системы из 
неперегруженного состояния в перегруженное и обратно. В начале главы 
упоминалось, что данному поведению системы уделяется особое внимание при 
прогнозировании. 

Пример расписания активных потоков изображен на рисунке \ref{fig:testplan}, где 
по оси $X$ -- время прошедшее с начала теста, а по оси $Y$ -- количество 
потоков.

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{\detokenize{test_plan}}
	\caption{График количества активных потоков в расписании тестового 
															плана.}
	\label{fig:testplan}
\end{figure} 

\section{Сравнение моделей прогнозирования ARIMA}
Ранее в главе был дан краткий обзор моделей прогнозирования ARIMA, и как 
можно отметить, один из основных недостатков данных моделей -- это 
необходимость повторного расчета коэффициентов при получении новых 
данных, что может негативно сказаться на производительности системы (так 
как значимая доля её процессорного времени будет уделяться данной 
проблеме) \cite{tihonov2006}. Поэтому порядок моделей и количество обучающих данных были ограничены. 

Для расчета ARIMA-моделей была использована Java-библиотека с открытым 
исходным кодом -- Workday/timeseries-forecast. На одних и тех же входных 
данных по нагрузке процессора, собранных за один час работы системы 
(процесс сбора данных описан в предыдущем разделе), были построены модели 
с порядками $p \le 11$, $d \le 3$ и $q \le 11$. Количество обучающих данных n 
варьировалось от 10 до 4000. 

Оценка точности модели производилась по следующему алгоритму: 
\begin{itemize}
	\item рассчитывалась ARIMA модель с порядками $p$, $d$ и $q$ на 
		последовательных значениях нагрузки с $i$-го по $i + n - 1$ 
	\item рассчитывался прогноз $f$ следующего шага -- $i + n$
	\item происходила нормализация спрогнозированного значения по
		следующему правилу: 
				если $f < 0$, то $f_n = 0$; если $f > 1$, то $f_n = 1$
	\item фиксировалась ошибка -- разница между фактическим значением 
		временного ряда и нормализованным значением $f_n$
	\item выполненные действия повторялись при $i = i + 1$
\end{itemize}
После завершения работы данного алгоритма вычислялась средняя 
квадратическая ошибка (root mean square error RMSE, данный способ оценки 
точности часто используется для сравнения моделей пронозирования 
\cite{liu1994, mills2011, hyndman2012}) модели порядков $p$, $d$, $q$ с 
количеством обучающих данных равным $n$. Результаты, отсортированные в 
порядке возрастания ошибки, представлены в таблицах 
\labelcref{tab:mamodels,tab:armodels,tab:armamodels,tab:arimamodels}. 
Результаты прогнозов наиболее точных моделей AR, MA, ARMA и ARIMA 
изображены на рисунке \ref{fig:arimaforecasting}.

\begin{gosttable}
	\caption{Сравнение моделей MA.}
	\begin{tabular}{| >{\centering}m{3.5cm} | >{\centering}m{3cm} | 
									>{\centering\arraybackslash}m{3.5cm} |}
		\hline
		Порядок модели MA ($q$) 
			& Количество обучающих данных ($n$)
			& Cредняя квадратическая ошибка \\ 
		\hline
		1 & 15 & 0.116628 \\ \hline
		1 & 16 & 0.116784 \\ \hline
		1 & 11 & 0.116977 \\ \hline
		1 & 13 & 0.117005 \\ \hline
		1 & 18 & 0.117108 \\ \hline
		1 & 20 & 0.117462 \\ \hline
		1 & 19 & 0.117549 \\ \hline
		1 & 14 & 0.117555 \\ \hline
		1 & 17 & 0.117649 \\ \hline
		1 & 22 & 0.117656 \\ \hline
	\end{tabular}
	\label{tab:mamodels}
\end{gosttable}

\begin{gosttable}
	\caption{Сравнение моделей AR.}
	\begin{tabular}{| >{\centering}m{3.5cm} | >{\centering}m{3cm} | 
									>{\centering\arraybackslash}m{3.5cm} |}
		\hline
		Порядок модели AR ($p$) 
			& Количество обучающих данных ($n$)
			& Cредняя квадратическая ошибка \\ 
		\hline
		9 & 800 & 0.103868 \\ \hline
		8 & 800 & 0.103885 \\ \hline
		8 & 700 & 0.103948 \\ \hline
		9 & 700 & 0.103956 \\ \hline
		10 & 800 & 0.103992 \\ \hline
		10 & 700 & 0.104136 \\ \hline
		7 & 800 & 0.104277 \\ \hline
		9 & 900 & 0.104283 \\ \hline
		8 & 900 & 0.104333 \\ \hline
		7 & 700 & 0.104349 \\ \hline
	\end{tabular}
	\label{tab:armodels}
\end{gosttable}

\begin{gosttable}
	\caption{Сравнение моделей ARMA.}
	\begin{tabular}{| >{\centering}m{3cm} | >{\centering}m{3cm} | 
									>{\centering\arraybackslash}m{3.5cm} |}
		\hline
		Порядки модели ARMA ($p, q$) 
			& Количество обучающих данных ($n$)
			& Cредняя квадратическая ошибка \\ 
		\hline
		8, 1 & 800 & 0.104518 \\ \hline
		9, 1 & 800 & 0.104555 \\ \hline
		10, 1 & 700 & 0.104639 \\ \hline
		9, 1 & 700 & 0.104718 \\ \hline
		8, 1 & 700 & 0.104766 \\ \hline
		10, 1 & 900 & 0.104776 \\ \hline
		10, 1 & 800 & 0.104785 \\ \hline
		9, 2 & 700 & 0.104795 \\ \hline
		9, 3 & 800 & 0.104845 \\ \hline
		9, 1 & 900 & 0.104894 \\ \hline
	\end{tabular}
	\label{tab:armamodels}
\end{gosttable}

\begin{gosttable}
	\caption{Сравнение моделей ARIMA.}
	\begin{tabular}{| >{\centering}m{4cm} | >{\centering}m{3cm} | 
									>{\centering\arraybackslash}m{3.5cm} |}
		\hline
		Порядки модели ARIMA ($p, d, q$) 
			& Количество обучающих данных ($n$)
			& Cредняя квадратическая ошибка \\ 
		\hline
		0, 1, 3 & 700 & 0.103599 \\ \hline
		7, 1, 0 & 700 & 0.103604 \\ \hline
		8, 1, 0 & 700 & 0.103613 \\ \hline
		8, 1, 0 & 800 & 0.103665 \\ \hline
		7, 1, 0 & 800 & 0.103714 \\ \hline
		7, 1, 0 & 600 & 0.103752 \\ \hline
		0, 1, 3 & 800 & 0.103774 \\ \hline
		8, 1, 0 & 600 & 0.103777 \\ \hline
		10, 1, 0 & 800 & 0.103798 \\ \hline
		9, 1, 0 & 700 & 0.103804 \\ \hline
	\end{tabular}
	\label{tab:arimamodels}
\end{gosttable}
\vspace{1em}
\begin{figure}[!h]
	\centering
	\includegraphics[width=\linewidth]{\detokenize{arima_forecasting}}
	\caption{Прогноз нагрузки процессора.}
	\label{fig:arimaforecasting}
\end{figure} 

\newpage
На рисунке \ref{fig:arimaforecasting} можно отметить, что резкие скачки нагрузки 
не прогнозируются ни одной из моделей. Кроме того наиболее точные модели 
вычисляют повторение исходного ряда, но смещенного по времени. По сути 
значения рассчитанные любой из вышеперечисленных моделей нельзя считать 
прогнозом. Совокупность данных фактов не соответствует основному 
требованию прогнозирования пиков нагрузки, приведённому в начале главы.

\section{Сравнение нейросетевых моделей} 
Ранее в главе были представлены два типа нейронных сетей, используемых для 
прогнозирования временных рядов. Следует выделить их особенности 
\cite{chernodub2012}:
\begin{enumerate}
	\item При использовании многослойного персептрона, в отличие от 
		рекуррентного, необходимо заранее знать порядок линии задержек на 
		входе. Однако, в пределах данной работы это не является проблемой, 
		так как порядок может быть выбран во время экспериментов, и 
		больше не меняться в ходе работы системы.
	\item Рекуррентные сети имеют б\'{о}льшую точность при многошаговом 
		прогнозировании, но это так же не имеет значения в данной работе.
	\item Обучение рекуррентных сетей является более трудоемкой задачей, в 
		виду наличия у них дополнительных степеней свободы 
		\cite{medsker2000, naseera2015}.
\end{enumerate}
Из этого следует, что при выборе между двумя типами нейронных сетей 
наибольший интерес (на основании теоретических сведений) представляет 
первый. 

Но наличие рекуррентных связей является не единственной структурной 
конфигурацией, приводящим к изменению точности прогнозирования. Влияние 
оказывают и другие параметры, такие как: количество слоёв, количество  
нейронов в скрытых слоях, а так же активационные функции нейронов. Более  
того, при изменении алгоритма обучения можно добиться различной 
сходимости, а следовательно -- и точности \cite{naseera2015}.

Для составления и обучения нейронной сети был использован Java"=фреймворк 
с открытым исходным кодом -- Encog. В качестве обучающего был взят тот же 
участок данных, что использовался при построении моделей ARIMA в 
предыдущем разделе. На вход нейронной сети подавались значения семнадцати 
динамических характеристик в момент времени $t$, а также $n - 1$ предыдущих 
значений каждой из характеристик для сети с линией задержек порядка $n$ 
(более подробное описание данных характеристик представлено в разделе 
\ref{sec:datapreparation}). Если при $t + 1$ система находилась в состоянии 
перегрузки, то на выходе сети ожидалось значение 1. В обратном случае -- 0.

Всего было обучено около десяти тысяч нейронных сетей различных 
конфигураций. При этом количество скрытых слоёв варьировалось от одного до 
трёх, а количество нейронов в скрытых слоях от одного до количества нейронов 
предыдущего слоя. Кроме того, изменялись функции активации скрытого и 
выходного слоёв (всего использовалось 3 типа функций: сигмоидальная, 
гиперболического тангенса и логарифмическая), а так же порядок линии 
задержек от одного до десяти для нейронной сети с линией задержек. При 
обучении использовался метод Resilient Propagation (упругого распространения), 
как показавший наибольшую точность при решении подобных задач 
\cite{naseera2015}. Обучение продолжалось до двадцати тысяч эпох, но при 
обучении рекуррентной нейронной сети не удалось добиться схождения.

Суммарно на обучение нейронных сетей было затрачено около трёх недель. В 
виде тестовых данных был взят еще один участок статистических данных 
длиной один час. Точность оценивалась по факту значения ошибки -- числа 
неверно спрогнозированных бинарных значений относительно общего числа 
прогнозов на всех обучающих и тестовых данных. Значение ошибки для десяти 
лучших конфигураций нейронных сетей можно увидеть в таблице 
\ref{tab:nnmodels}.

\begin{gosttable}
	\caption{Сравнение нейросетевых моделей.}
	\begin{tabular}{| >{\centering}m{4cm} | >{\centering}m{3cm} | 
									>{\centering\arraybackslash}m{3.5cm} |}
		\hline
		0 & 0 & 0 \\
		\hline
	\end{tabular}
	\label{tab:nnmodels}
\end{gosttable}

Было отмечено, что при добавлении тестовых выборок в обучающие данные 2-х 
и 3-х слойных нейронных сетей с линиями задержек 4, 5 и 6 порядков из 
таблицы \ref{tab:nnmodels}, процесс обучения сходится и процент общей ошибки 
уменьшается. %после формирования таблицы здесь так же появится вывод о 
%том, какие нейронные сети соответствуют требованию, представленному в 
%начале главы

\section{Результаты сравнительного анализа} %just a sketch
\begin{enumerate}
	\item Модели AR, MA, ARMA, ARIMA не подходят для прогнозирования 
		нагрузки процессора, в силу недостаточной точности на данном виде 
		временного ряда.
	\item Модель прогнозирования, составленная на базе многослойного 
		пересептрона, имеет небольшой процент ошибки на обучающей 
		выборке и будет использоваться при разработке алгоритма 
		распределения нагрузки.
\end{enumerate}

\chapter{Разработка алгоритма распределения нестационарной нагрузки}
\section{Внедрение нейронной сети в разрабатываемый сервис}
После внедрения алгоритма распределения запросов поведение 
разрабатываемого сервиса изменится, в силу следующих причин:
\begin{enumerate}
	\item Каждые 300 мс. вместе с получением новых данных будет 
		рассчитываться прогноз, а точнее производиться перемножение 
		весовых коэффициентов каждого слоя нейронной сети на значения 
		полученные с функций активации предыдущих слоев. Данное действие 
		займет часть процессорного времени вычислительной системы.
	\item Если нейронная сеть спрогнозирует перегрузку системы, то 
		результаты поиска не будут загружаться из внешних ресурсов и 
		пользователи сразу же получат результаты найденные в кэше. В 
		таком случае нагрузка системы снизится. 
\end{enumerate}
Поэтому данные необходимые для обучения нейронной сети могут быть 
получены только при её функционировании в составе сервиса. Тогда 
на вход нейронной сети с линией задержек порядка $n$ так же может 
подаваться восемнадцатый временной ряд -- предыдущие $n$ бинарных 
значений с выхода нейронной сети. Анализ данного ряда позволит определить 
снижение нагрузки, произошедшее в предыдущие моменты времени. 

Для внедрения нейронной сети в разрабатываемый сервис, необходимо 
разработать алгоритм аккумулирования обучающей выборки нейронной сети, 
который позволит не перегружать обучающую выборку слишком большим 
объемом данных, и в то же время постепенно увеличивать точность 
прогнозирования в процессе обучения. 

\section{Разработка алгоритма аккумулирования обучающей выборки} %just a sketch
Так как днем нагрузка оказываемая на сервис может быть достаточно велика, 
обучение нейронной сети должно производиться ночью. При этом интервал 
обучения может быть ограничен. 

%или программная?
\section{Практическая реализация разработанного алгоритма распределения 
											нестационарной нагрузки}

\chapter{Тестирование разработанного алгоритма}

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{\detokenize{stand_after}}
	\caption{Схема тестового стенда.}
	\label{fig:standafter}
\end{figure} 

\likechapter{Заключение}
\printbibliography[heading=bibheading]
 
\end{document}
