\documentclass[a4paper,14pt,russian]{extreport}

\usepackage{extsizes}
\usepackage{cmap}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{pscyr}
\usepackage{indentfirst}
\renewcommand{\rmdefault}{ftm}
\frenchspacing
\usepackage[nodisplayskipstretch]{setspace}
\onehalfspacing

\usepackage{graphicx}
\graphicspath{ {images/} } 

\usepackage{amssymb,amsfonts,amsmath,amsthm}
%\usepackage[usenames,dvipsnames]{color}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{array,makecell,booktabs}
\usepackage{adjustbox}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{syntax}
\usepackage{listings}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[C]{\thepage}
\fancyheadoffset{0mm}
\fancyfootoffset{0mm}
\setlength{\headheight}{17pt}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\fancypagestyle{plain}{ 
	\fancyhf{}
	\chead{\thepage}}
\setcounter{page}{4}

\renewcommand{\arraystretch}{1.4}
\setlength{\floatsep}{2em}

\usepackage[tableposition=top]{caption}
\usepackage{subcaption}
\DeclareCaptionLabelFormat{gostfigurelabel}{Рисунок #2}
\DeclareCaptionLabelFormat{gosttablelabel}{Таблица #2}
\DeclareCaptionLabelSeparator{gost}{~---~}
\captionsetup{labelsep=gost}
\captionsetup[figure]{labelformat=gostfigurelabel}
\captionsetup[table]
	{labelformat=gosttablelabel, justification=raggedright, singlelinecheck = false}
\usepackage{threeparttable}
\renewcommand{\thesubfigure}{\asbuk{subfigure}}

\usepackage{cleveref}

\usepackage{geometry}
\geometry{left=2.5cm}
\geometry{right=1.0cm}
\geometry{top=2.0cm}
\geometry{bottom=2.0cm}

	\usepackage{titlesec}
	 
	\titleformat{\chapter}
		{\filcenter\bfseries}
		{\thechapter}
		{8pt}
		{\MakeUppercase}
	 
	\titleformat{\section}
		{\normalsize\bfseries}
		{\thesection}
		{1em}{}
	 
	\titleformat{\subsection}
		{\normalsize\bfseries}
		{\thesubsection}
		{1em}{}
	
	\titlespacing*{\chapter}{0pt}{-30pt}{8pt}
	\titlespacing*{\section}{\parindent}{*4}{*4}
	\titlespacing*{\subsection}{\parindent}{*4}{*4}
	
	\usepackage{enumitem}
	\makeatletter
	    \AddEnumerateCounter{\asbuk}{\@asbuk}{м)}
	\makeatother
	\renewcommand{\labelenumi}{\arabic{enumi}.}
	\renewcommand{\labelenumii}{\asbuk{enumii}.}
	
	\usepackage{tocloft}
	\renewcommand{\cfttoctitlefont}{\hspace{0.38\textwidth} \bfseries\MakeUppercase}
	\renewcommand{\cftaftertoctitleskip}{2em}
	\renewcommand{\cftbeforetoctitleskip}{-1em}
	\renewcommand{\cftchapfont}{\normalsize\bfseries}
	\renewcommand{\cftdotsep}{1}
	\setcounter{tocdepth}{2}
	
	\setlength\cftbeforechapskip{1em}
	\setlength\cftbeforesecskip{.5em}
	\setlength\cftbeforesubsecskip{.25em}
	
	\newcommand{\empline}{\vspace{1em}}
	\newcommand{\likechapterheading}[1]{ 
		\clearpage   
		\begin{center}
		\textbf{\MakeUppercase{#1}}
		\end{center}
	}

\usepackage{etoolbox}
	\makeatletter
		\patchcmd{\l@chapter}{#1}{\MakeUppercase{#1}}{}{}
		\renewcommand{\@dotsep}{1}
		\newcommand{\l@likechapter}[2]
			{{\bfseries\@dottedtocline{0}{0pt}{0pt}{#1}{\bfseries#2}}}
	\makeatother
	\newcommand{\likechapter}[1]{ 
		\likechapterheading{#1}    
		\addtocontents{toc}{\vspace{1em}}
		\addcontentsline{toc}{likechapter}{\MakeUppercase{#1}}
		\empline
	}
	
%	\usepackage[
%		backend=biber,
%		bibencoding=utf8,
%		sorting=none,
%		style=gost-numeric,
%		autolang=none,
%		eprint=false
%	]{biblatex}

\bibliographystyle{utf8gost705u}
\makeatletter % [1] -> 1.
\renewcommand\@biblabel[1]{#1.}
\makeatother
%\addto\captionsrussian{\def\refname{Библиографический список}}
\addto\captionsrussian{\renewcommand{\bibname}{Библиографический список}}

\newcommand{\url}[1]{#1}

% \defbibheading{bibheading}[Библиографический список]{
% 	\likechapterheading{#1}
% 	\addtocontents{toc}{\vspace{1em}}
% 	\addcontentsline{toc}{likechapter}{\MakeUppercase{#1}}
% }
%\DeclareFieldFormat{author}{{#1}}

\usepackage{lastpage}

	\usepackage[title,titletoc]{appendix}
	 
	\titleformat{\paragraph}[display]
		{\filcenter}
		{\MakeUppercase{\chaptertitlename} \thechapter}
		{8pt}
		{\bfseries}{}
	\titlespacing*{\paragraph}{0pt}{-30pt}{8pt}
	 
	\newcommand{\append}[1]{  
		\clearpage
		\stepcounter{chapter}    
		\paragraph{\MakeUppercase{#1}}
		\empline
		\addcontentsline{toc}{likechapter}
					{\MakeUppercase{\chaptertitlename~\Asbuk{chapter}\;#1}}}

\setlength{\parindent}{1.25cm} 
\setlength{\parskip}{6pt}


\newenvironment{gosttable}
	{
		\begin{table}[!h]
			\centering
			\begin{adjustbox}{max width=\textwidth}
				\begin{threeparttable}
	}	
	{
				\end{threeparttable}
			\end{adjustbox}
		\end{table}
	}
	
\newenvironment{eqwhere}
	{где:
		\par\noindent\hspace{2em}\begin{tabular}{>{$}r<{$} @{${}$ -- {}} l}
	}
	{\end{tabular}\par\vspace{\belowdisplayskip}}

\begin{document}

\tableofcontents

\likechapter{Введение}

{\bfseries Актуальность темы исследования.} 
В последнее время большое распространение получили различные 
сервисы-агрегаторы, объединяющие данные из нескольких источников
определённой тематики в один, что уменьшает затрачиваемое пользователем
время на поиск необходимой информации. 

Для обработки запроса от одного пользователя, агрегатору может 
потребоваться совершить десятки или даже сотни запросов к сторонним 
ресурсам. В силу того, что пользователи могут запрашивать одну и ту же 
информацию, самым распространённым способ снижения времени ответа 
является использование промежуточных буферов, так называемых кэшей. Но 
данный способ приводит к снижению актуальности предоставляемых 
агрегатором данных и может быть использован для предупреждения 
избыточной нагрузки системы, либо для хранения редко обновляемых данных, 
таких как словари. Словарь -- это набор статических данных используемых 
агрегатором для объединения информации получаемой с нескольких источников 
(например, для агрегатора отелей словарём будет являться список всех отелей 
по которым осуществляется поиск). 

Таким образом, разработка алгоритма распределения нестационарной нагрузки, 
определяющего источник данных агрегатора, является актуальной задачей. 
Однако для её решения необходим анализ или прогноз нагрузки системы.

{\bfseries Степень теоретической разработанности темы.}
В открытом доступе существует множество научных работ, описывающих 
методы прогнозирования временных рядов. Но материалы, описывающие 
проблему выбора метода прогнозирования для разработки алгоритма 
распределения нестационарной нагрузки, найти не удалось. Из сказанного выше 
можно сделать вывод о том, что рассматриваемая тема имеет низкую степень 
теоретической проработанности.

{\bfseries Объектом исследования} является прогнозирование нестационарной 
нагрузки сервиса"=агрегатора.

{\bfseries Предметом исследования} являются методы прогнозирования 
временных рядов.

{\bfseries Область исследования.} Проведённое исследование методов 
прогнозирования нагрузки вычислительной системы в пределах разработки 
алгоритма распределения запросов пользователей полностью соответствует 
специальности «Вычислительные машины, комплексы, системы и сети», а 
содержание выпускной квалификационной работы -- техническому
заданию.

{\bfseries Цель и задачи исследования.}  Целью работы является улучшение 
качества обслуживания пользователей сервиса-агрегатора за счет снижения 
среднего времени ответа.

Для достижения данной цели были поставлены следующие задачи:
\begin{enumerate}
	\item Выполнить сравнительный анализ методов прогнозирования.
	\item Разработать алгоритм распределения нестационарной нагрузки и его 
		программную реализацию на основе выбранного метода 
		прогнозирования.
	\item Выполнить сравнение среднего времени ответа исходной и 
		использующей разработанный алгоритм систем.
\end{enumerate}

{\bfseries Теоретическую основу исследования} составляют научные труды
отечественных и зарубежных авторов в области компьютерных технологий и
математической статистики.

{\bfseries Методологическую основу исследования} составляет эксперимент.

{\bfseries Научная новизна работы} заключается в следующем:
\begin{enumerate}
	\item В настоящее время не существует структурированных рекомендаций 
		по выбору метода прогнозирования при разработке алгоритмов 
		распределения нагрузки, представленных в данной работе. 
	\item Разработанный алгоритм распределения нестационарной нагрузки 
		может обеспечить улучшение качества обслуживания пользователей
		сервиса"=агрегатора.
\end{enumerate}

{\bfseries Практическая значимость} данной работы заключается в том, что 
разработанный алгоритм распределения нестационарной нагрузки будет
интегрирован в разрабатываемый сервис"=агрегатор. Кроме того, 
сформулированные рекомендации могут быть использованы для осуществления 
выбора метода прогнозирования при разработке собственного алгоритма 
распределения запросов пользователей.

{\bfseries Апробация результатов исследования.} Сформулированные 
рекомендации по выбору метода прогнозирования обсуждались на VII Конгрессе 
молодых учёных, а их сравнительный анализ был представлен на XLVII научной 
и учебно-методической конференции.

{\bfseries Объем и структура работы.} 
Выпускная квалификационная работа содержит 40 страниц машинописного 
текста, восемь рисунков, семь таблиц, девять формул и список литературы, 
включающий 25 источников. Структурно работа состоит из введения, трёх 
разделов и заключения. Во введении обоснована актуальность выбранной темы, 
а также новизна исследования. Определены цели и задачи, объект и предмет 
исследования. Представлена апробация результатов исследования. Первый 
раздел содержит обзор предметной области, рассматриваются широко 
используемые методы прогнозирования временных рядов. Проводится 
сравнительный анализ методов и формулируются рекомендации по выбору 
моделей прогнозирования. Второй раздел посвящён процессу разработки 
алгоритма распределения нестационарной нагрузки, в нём описывается 
составление требований для нового алгоритма, а также проблемы его 
разработки. Приведено описание разработанных алгоритмов аккумуляции 
обучающей выборки и распределения запросов пользователей. Третий раздел 
посвящен тестированию разработанного комплекса. Приводятся результаты 
тестирования под двумя типами нагрузок. В заключении сформулированы 
основные результаты работы.

\chapter{Сравнительный анализ методов прогнозирования}
Необходимость в точном прогнозе возникает во многих областях науки, 
индустрии, коммерческой и экономической деятельностях. Ситуации его 
использования весьма разнообразны, и в зависимости от исходной проблемы, 
прогнозирование может осуществляться вперед на несколько лет, а может на 
несколько секунд. 

Прогнозирование является важной составляющей эффективного планирования 
ресурсов. В качестве примера может быть приведено прогнозирование 
электрических нагрузок в электроэнергетике: на основе предсказанных величин 
рассчитываются оптимальные режимы электро"=энергетических систем, а 
точность прогноза влияет на экономичность загрузки генерирующего 
оборудования, и, следовательно, на стоимость электроэнергии.

Метод прогнозирования - это процедура вычисления прогноза из текущих и 
прошлых значений. Это может быть лишь алгоритмическое правило не 
зависящее от базовой вероятностной модели, поведение которой необходимо 
предсказать. В другом случае, метод может использоваться с целью 
определения конкретной модели для предоставленных данных или поиска 
оптимальных условий прогнозирования для такой модели. Поэтому стоит 
различать понятия <<модель>> и <<метод>> \cite{chatfield2000}. 

Существует широкий спектр методов прогнозирования, по оценкам зарубеж-
ных и отечественных систематиков прогностики их насчитывается
уже свыше ста \cite{tihonov2006}. Каждый имеет собственные характеристики 
(такие как точность или вычислительная сложность), которые должны быть 
учтены при выборе метода.

Методы прогнозирования можно разделить на три основных типа 
\cite{chatfield2000, armstrong1999, brockwell2002, hyndman2012}:
\begin{enumerate}
	\item {\itshape Методы суждений} основанные на субъективных суждениях, 
		интуиции, <<внутренних>> коммерческих знаниях, или любой другой 
		подобной информации.
	\item {\itshape Одномерные методы} где предсказание зависит только от 
		прошлых и текущего значений предсказываемого ряда.
	\item {\itshape Многомерные методы} в которых прогнозирование величины, 
		по крайней мере частично, так же зависит от одного или б\'{о}льшего 
		количества временных рядов. Такую совокупность, состоящую из 
		нескольких рядов, называют многомерными временными рядами 
		\cite{popov2006}.
\end{enumerate}

Все рассматриваемые в данной главе методы осуществляли прогнозирование 
поведения временных рядов. Временной ряд - это ряд наблюдений, 
произведенных последовательно во времени \cite{chatfield2000, armstrong1999, 
brockwell2002, box2008, wolters2007, hyndman2012}. Он включает два 
обязательных элемента: время и конкретное значение показателя, или уровень 
ряда \cite{popov2006, afanasyev2001}. 

Временным рядом можно назвать огромное количество последовательностей, 
являющихся следствием измерений некоторого показателя. Например, 
показатели (характеристики) экономических, природных, промышленных, 
информационных и других систем. Фактически, временной ряд может быть 
построен тремя способами: путем отбора значений из непрерывного ряда, путем 
агрегирования данных за установленные периоды времени, и взятием серии 
дискретных наблюдений \cite{chatfield2000, box2008}. Для всех трех типов запись 
данных производится с равными промежутками времени.

Временные ряды подразделяются на стационарные и нестационарные. Ряд 
называется стационарным, если его среднее значение, дисперсия и ковариация 
не зависят от времени \cite{popov2006}. Другими словами, это ряд параметры 
которого не зависят от того, в какой момент времени производились замеры 
\cite{hyndman2012}. Ряд не соответствующий данным условиям называется 
нестационарным.

Одним из наиболее важных видов временных рядов является 
последовательность некоррелированных (не взаимосвязанных) случайных 
величин с нулевым математическим ожиданием и постоянной дисперсией 
\cite{fuller1996, runova2013}, такой ряд называется белым шумом.

Далее в главе производится сравнительный анализ наиболее популярных 
одномерных и многомерных методов прогнозирования. Методы суждений не 
рассматриваются в виду того, что их невозможно автоматизировать.

\section{Методы прогнозирования временных рядов, методология ARIMA} 
Среди методов прогнозирования наибольшее распространение получила 
методология Бокса -- Дженкинса или ARIMA (autoregressive integrated moving 
average: интегрированная модель авторегрессии -- скользящего среднего). Класс 
моделей ARIMA - это важный инструмент прогнозирования и базис многих 
фундаментальных идей в анализе временных рядов \cite{chatfield2000}. Он 
удобен тем, что сочетает в себе модели ARMA, AR, MA и может использоваться 
для прогнозирования как стационарных так и нестационарных временных рядов. 
Данным классом учитывается связь между случайными остатками временного 
ряда, которые можно получить вычтя из исходного ряда его неслучайную 
составляющую (тренд) \cite{tihonov2006}. Далее в разделе представлено описание 
моделей.


\subsection{Модели скользящего среднего MA(q)}
В моделях скользящего среднего текущее значение ряда представляется в виде 
линейной комбинации текущего и прошедших значений ошибки $\varepsilon_t$, 
$\varepsilon_{t-1}$, $...$, $\varepsilon_{t-q}$, по своим свойствам соответствующей 
<<белому шуму>>. Модель скользящего среднего порядка $q$ 
может быть выражена следующим уравнением \cite{runova2013}:
\begin{equation}
	y_t = \varepsilon_t - \gamma_1 \varepsilon_{t-1} - \gamma_2 \varepsilon_{t-2} 
		- ... - \gamma_q \varepsilon_{t-q}, 
\end{equation} 
\begin{eqwhere}
	y_t	& значение ряда в момент времени $t$ \\
	\gamma_1, \gamma_2, ..., \gamma_q 	& параметры модели, \\
	\varepsilon_t	& случайные ошибки образующие <<белый шум>>.\\
\end{eqwhere}
В эквивалентной форме:
\begin{equation}
	y_t = (1 - \gamma_1 B - \gamma_2 B^2 - ... - \gamma_q B^q)\varepsilon_t \text{,} 
\end{equation}
или:
\begin{equation}
	y_t = \gamma(B)\varepsilon_t \text{,} 
\end{equation}
где $B$ - оператор сдвига назад:
\begin{equation}
	B y_t = y_{t-1} \text{.} \nonumber
\end{equation}

Таким образом, процесс скользящего среднего можно трактовать как выход 
$y_t$ линейного фильтра с передаточной функцией $\gamma(B)$, на вход 
которого подается процесс белого шума $\varepsilon_t$ \cite{box2008}.


\subsection{Модели авторегрессии AR(p)}
Модель авторегрессии порядка $p$ может быть представлена в следующем 
виде \cite{runova2013}:
\begin{equation}
	y_t = \beta_1 y_{t-1} + \beta_2 y_{t-2} + ... + \beta_p y_{t-p} + \varepsilon_t \text{,} 
\end{equation}
где $\beta_1, \beta_2, ..., \beta_p$ -- параметры модели. Таким образом 
прогнозируемое значение представляется в виде линейной зависимости от $p$ 
известных значений временного ряда. 

В эквивалентной форме:
\begin{equation}
	y_t = \frac{\varepsilon_t}{1 - \beta_1 B - \beta_2 B^2 - ... - \beta_p B^p} \text{,} 
\end{equation}
или:
\begin{equation}
	y_t = \beta^{-1}(B)\varepsilon_t \text{,}
\end{equation}
отсюда процесс авторегрессии может быть интерпретирован как выход $y_t$ 
линейного фильтра с передаточной функцией $\beta^{-1}(B)$, на вход которого 
подается процесс белого шума $\varepsilon_t$ \cite{box2008}. 


\subsection{Модели авторегрессии -- скользящего среднего ARMA(p, q)}
Конечный процесс авторегрессии может быть представлен как бесконечный 
процесс скользящего среднего MA($\infty$) \cite{hamilton1994, chatfield2000}, 
однако с увеличением порядка модели её расчет значительно усложняется и 
потому если процесс действительно типа AR, то его представление в виде 
скользящего среднего не может быть экономичным. Точно так же процесс MA не 
может быть экономично представлен с помощью процесса авторегресии. Для 
того, чтобы сделать параметризацию более экономичной в модель могут быть 
включены не только члены моделирующие скользящее среднее, но и члены 
описывающие авторегрессию.

Общий вид авторегрессии -- скользящего среднего ARMA(p, q) определяется 
следующим уравнением \cite{runova2013}:
\begin{equation}
	y_t = \beta_1 y_{t-1} + \beta_2 y_{t-2} + ... + \beta_p y_{t-p} + \varepsilon_t 
		- \gamma_1 \varepsilon_{t-1} - \gamma_2 \varepsilon_{t-2}  - ... 
		- \gamma_p \varepsilon_{t-p}, 
\end{equation}
Как легко заметить, при порядке $p$ равном нулю будет получен процесс MA, 
а при обнулении порядка $q$ - процесс AR. 

Процесс  может быть записан в эквивалентной форме:
\begin{equation}
	y_t = \frac
		{1 - \gamma_1 B - \gamma_2 B^2 - ... - \gamma_q B^q}
		{1 - \beta_1 B - \beta_2 B^2 - ... - \beta_p B^p}  \varepsilon_t \text{,} 
\end{equation}
то есть, смешанный процесс авторегрессии -- скользящего среднего 
может быть истолкован как выход $y_t$ линейного фильтра, с передаточной 
функцией в виде отношения двух полиномов, на вход которого подается белый 
шум $\varepsilon_t$ \cite{box2008}. 
 
 \subsection{Модели авторегрессии -- проинтегрированного скользящего среднего 
 ARIMA(p, d, q)}
 Перечисленные выше модели используются для прогнозирования стационарных 
 процессов. Однако, существуют временные ряды, среднее значение которых 
 может изменяться в течение времени. Однако даже они могут демонстрировать 
 однородность. И, возможно, если не учитывать локальный уровень, или 
 локальный уровень и тренд, то любая часть подобного временного ряда может 
 по своему поведению оказаться подобна любой другой части. Сделав 
 предположение, что некая подходящая разность процесса стационарна, можно 
 получить модели, которые будут описывать подобное однородное 
 нестационарное поведение. Модели, в которых d-я разность есть стационарный 
 смешанный процесс авторегрессии -- скользящего среднего называются 
 процессами авторегрессии -- проинтегрированного скользящего среднего 
 ARIMA(p, d, q) \cite{box2008}. Данный процесс может быть представлен 
 уравнением вида:
 \begin{equation}
	\Delta^d y_t = \beta_1 \Delta^d y_{t-1} + \beta_2 \Delta^d y_{t-2} + ... 
		+ \beta_p \Delta^d y_{t-p} + \varepsilon_t  - \gamma_1 \varepsilon_{t-1} 
		- \gamma_2 \varepsilon_{t-2} - ... - \gamma_p \varepsilon_{t-p}, 
\end{equation}
где $\Delta^d$ --  оператор разности временного ряда порядка d, например: 
\begin{equation}
	\Delta y_t = y_t - y_{t - 1} \text{,} \nonumber
\end{equation}
для первого порядка;
\begin{equation}
	\Delta^2 y_t = \Delta y_t - \Delta y_{t - 1} = (y_t - y_{t - 1}) - (y_{t - 1} - y_{t - 2}) 
		= y_t - 2y_{t - 1} + y_{t - 2} \text{,} \nonumber
\end{equation}
для второго.

\section{Нейросетевые методы прогнозирования}
Искусственные нейронные сети (далее - нейронные сети) возникли на основе 
знаний о функционировании нервной системы живых существ. Они 
представляют собой попытку использования процессов, происходящих в 
нервных системах для выработки новых технологических решений 
\cite{osovskiy2002}. 

Основным элементом обработки информации в нейронной сети является модель 
нейрона. В его основе лежат \cite{haykin1999}:
\begin{enumerate}
	\item Набор связей (connecting link) или синапсов (synapse), при этом 
		каждый синапс имеет свой вес (weight). Например, если на вход 
		синапса $j$, который связан с нейроном $k$, поступает сигнал $x_j$, то 
		этот сигнал умножается на вес $w_{k j}$. При этом веса могут иметь
		как положительное, так и отрицательное значение.
	\item Сумматор, который складывает все входные сигналы перемноженные 
		на соответсвующие им веса.
	\item Функция активация (activation function), ограничивающая амплитуду 
		выходного сигнала нейрона. Как правило, значение на выходе нейрона 
		лежит в интервале $[0, 1]$ или $[-1, 1]$. Обычно выделяют 3 основных 
		типа активационных функций: единичного скачка, кусочко-линейные и 
		сигмоидальные. Наибольшее распространение получили последние.
\end{enumerate}

Одним из важнейших преимуществ нейронных сетей является возможность 
обучения. Обучение можно рассмотреть как корректировку весов сети, 
выполняемую по обучающим примерам (или обучающим данным), в следствие 
которой сеть меняет свою реакцию на входные воздействия.

Следует отметить, что нейронные сети позволяют получить результат на ранее 
не виденных примерах данных. Поэтому нейросетевые методы хорошо себя 
зарекомендовали как средство моделирования динамических систем при 
неизвестной априори математической модели динамической системы. 
Существует два базовых метода наделения нейронных сетей свойствами, 
необходимых для прогнозирования поведения динамических систем: 
добавление линий задержек и добавление рекуррентных связей
\cite{chernodub2012}. Оба метода могут использоваться как одномерные или 
двумерные и потому представляют особый интерес в данной работе. Их 
описание представлено в разделе далее.

\subsection{Многослойный персептрон с линией задержек}
В многослойном персептроне (Multilayer Perceptron, MLP) с линией задержек все 
нейроны расположены слоями, при этом имеется один входной слой, один 
выходной и как минимум один скрытый слой. Пример схемы данной сети 
порядка N и одним скрытым слоем изображен на рисунке \ref{fig:mlparch}. Сеть 
содержит нейроны с линейной функцией активации во входном слое и нейроны 
с сигмоидальной функцией активации в скрытом и выходном слоях. Весовые 
коэффициенты задаются матрицами $W_1$ и $W_2$. На вход нейронная 
сеть получает текущее значение временного ряда $y(k)$, а так же задержанные 
значения $y(k-1)$, $y(k-2),$ ... $y(k-N)$, полученные при помощи элементов 
запаздывания $Z^{-1}$, $Z^{-2}$, ... $Z^{-N}$. По полученным данным сеть 
обучается делать прогноз следующего значения временного ряда $y(k+1)$.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{\detokenize{mlp_base_architecture}}
	\caption{Схема многослойного персептрона с линией задержек порядка N}
	\label{fig:mlparch}
\end{figure} 

\subsection{Рекуррентный многослойный персептрон}
Рекуррентный многослойный персептрон (Recurrent Multilayer Perceptron, RMLP) 
отличается от многослойного персептрона тем, что его выходные значения 
зависят не только от значений на входе в данный момент времени, но и от 
предыдущих входных значений или состояния сети. Поэтому данный вид 
нейронных сетей получил широкое распространение в управляющих 
приложениях и приложениях направленных на обработку сигналов 
\cite{medsker2000}. Схема персептрона с элементами задержки первого порядка, 
используемого для прогнозирования временного ряда, изображена на рисунке 
\ref{fig:rmlparch}. Как можно заметить, в матрице весов скрытого слоя $W_1$ 
теперь хранятся еще и веса каждой из рекуррентных связей.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{\detokenize{rmlp_base_architecture}}
	\caption{Схема рекуррентного многослойного персептрона}
	\label{fig:rmlparch}
\end{figure}

\section{Сравнение моделей прогнозирования}
Так как выбранный в результате сравнительного анализа метод будет 
использоваться для предупреждения избыточной нагрузки, то ко всем моделям в 
данном разделе предъявлялось требование: ошибка предсказания пиков 
избыточной нагрузки (то есть перегруженного состояния системы, при 
отсутствие перегрузки на момент расчета прогноза) должна составлять не более 
5 \%.  

\subsection{Подготовка данных для сравнения} 
											\label{subsec:datapreparation}
Для сбора данных был построен тестовый стенд, включающий в себя две 
вычислительные системы, находящихся в одной локальной сети. Первая под 
управлением операционной системы Ubuntu 16.04.3 LTS, вторая под управлением 
операционной системы macOS High Sierra 10.13.3. Аппаратное обеспечение 
второй вычислительной системы:
\begin{itemize}
	\item {\itshape центральный процессор} Intel Core i5-4258U3,
	\item {\itshape оперативная память} 8 GB 1600 MHz DDR3,
	\item {\itshape внешняя память} SSD 128 GB.
\end{itemize}

На вычислительных ресурсах второй системы был развернут разрабатываемый 
сервис агрегатор, а так же система управления базами данных для его работы. 
Сервис принимал HTTP"=запросы, осуществлял необходимые для их 
выполнения действия и выдавал результаты. Кроме того, сервис в режиме 
реального времени осуществлял сбор статистических данных и сохранял их в 
файле в формате tsv (tab separated values -- значения, разделённые табуляцией). 
Схема тетового стенда изображена на рисунке \ref{fig:standbefore}. 

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{\detokenize{stand_before}}
	\caption{Схема тестового стенда}
	\label{fig:standbefore}
\end{figure} 

Так как сервис разрабатывается на Java, сбор информации о загрузке 
процессора осуществлялся при помощи компонента Java платформы 
\url{com.sun.management.OperatingSystemMXBean}. Данный компонент позволяет 
получить информацию о загрузке процессора от операционной системы 
\cite{osmxbean}. При помощи компонента \url{java.lang.Runtime} сервисом 
производился сбор информации об объёме памяти доступной виртуальной 
машине Java \cite{javaruntime}.

Каждые 300 мс сервисом производился сбор семнадцати статистических 
характеристик, которые возможно разделить на пять типов:
\begin{enumerate}
	\item {\bfseries Количество пришедших запросов с момента предыдущего 
														замера.\\}
		У разрабатываемого сервиса насчитывается шесть видов запросов 
		пользователей, для каждого выделено по одному отдельному  
		счетчику.
	\item {\bfseries Количество загрузчиков внешних ресурсов.\\}
		При поисковом запросе производится запуск процесса загрузки для 
		необходимых внешних ресурсов. По одной количественной 
		характеристике выделено для каждого внешнего ресурса, всего две.
	\item {\bfseries Среднее время обработки запроса с предыдущего замера по 
										текущий момент времени.\\}
		Аналогично первому типу -- шесть видов запросов.
	\item {\bfseries Загрузка процессора.\\}
		Две отдельные характеристики: загрузка процессора, оказываемая 
		процессом и общая загрузка процессора.
	\item {\bfseries Объем доступной памяти.\\}
		Одна характеристика: объем памяти доступной виртуальной машине 
		Java. 
\end{enumerate}
Характеристики первого и второго типа предоставляют информацию о том, на 
что именно расходуются вычислительные ресурсы системы. По общей 
тенденции данных временных рядов может быть предсказан факт снижения или 
увеличения нагрузки системы.
Характеристики третьего и четвёртого типов широко используются в 
нагрузочном тестировании как основные показатели производительности 
системы \cite{molyneaux2015} и потому могут быть использованы при 
прогнозировании её перегрузки. 
На основании значений пятой характеристики может быть спрогнозирован 
запуск сборщика мусора виртуальной машиной Java, данное действие оказывает 
значительное влияние на производительность системы \cite{hunt2016}. 
Графики со статистическими данными изображены на рисунке \ref{fig:sdcharts}.

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{\detokenize{statistic_data_charts}}
	\caption{Статистические данные сервиса-агрегатора}
	\label{fig:sdcharts}
\end{figure} 

Генерация HTTP-запросов осуществлялась средствами Apache JMeter. Был 
составлен тестовый план, потоки которого имитировали работу браузера 
пользователя, производя следующие действия:
\begin{enumerate}
	\item Запрос начальной информации, необходимой для отображения 
		стартовой страницы.
	\item Отправка запросов, имитирующих <<живой поиск>> по словарям 
		агрегатора. 
	\item Запрос на поиск со случайными параметрами.
	\item Отправка периодичных запросов в ожидании завершения поиска.
	\item Отправка запросов на получение результатов поиска.
\end{enumerate}
Временные интервалы между запросами устанавливались согласно правилам: 
\begin{itemize}
	\item среднее время позиционирования курсора и нажатия на клавишу 
		<<мыши>> составляет 0.4 с. \cite{korablev2011},
	\item cреднее время ввода символа при помощи клавиатуры составляет 
		0.25 с. \cite{korablev2011},
	\item при ожидании завершения поиска период отправки запросов 
		составляет 1 с.
\end{itemize}
Варьирование нагрузки производилось путем изменения числа одновременно 
исполняемых потоков. Было произведено стресс"=тестирование с целью 
определения числа потоков, приводящего систему к перегрузкам. Считалось, 
что система подвержена избыточной нагрузке, если среднее время выполнения 
любого из шести видов запросов составляло больше 90 мс. Во время 
тестирования каждые 2 часа происходило увеличение активных потоков на 10, 
затем рассчитывался процент времени перегрузки системы (например при 470 
активных потоках система находилась в состоянии перегрузки 6.31 \% времени). 
Всего на тестирование было затрачено около трёх суток. 

Был написан awk-скрипт, генерирующий расписание активных потоков для 
плагина Apache JMeter -- Ultimate Thread Groups. При генерации соблюдалось 
ограничение на число активных потоков: во время стрессового тестирования 
было установлено, что при 560 активных потоках система более 95 \% (98.14 \%) 
времени находилась в состоянии перегрузки, поэтому большее количество 
потоков не использовалось. Следует отметить, что все расписание может быть 
условно разделено на фазы трех типов: возрастание числа активных потоков, 
удержание числа потоков на постоянном уровне и уменьшение числа потоков. 
Посредством переключения фаз производится перевод системы из 
неперегруженного состояния в перегруженное и обратно. В начале главы 
упоминалось, что данному поведению системы уделяется особое внимание при 
прогнозировании. 

Пример расписания активных потоков изображен на рисунке \ref{fig:testplan}, где 
по оси $X$ -- время прошедшее с начала теста, а по оси $Y$ -- количество 
потоков.

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{\detokenize{test_plan}}
	\caption{График количества активных потоков в расписании тестового 
															плана}
	\label{fig:testplan}
\end{figure} 

\subsection{Сравнение моделей прогнозирования ARIMA}
Ранее в главе был дан краткий обзор моделей прогнозирования ARIMA, и как 
можно отметить, один из основных недостатков данных моделей -- это 
необходимость повторного расчета коэффициентов при получении новых 
данных, что может негативно сказаться на производительности системы (так 
как значимая доля её процессорного времени будет уделяться данной 
проблеме) \cite{tihonov2006}. Поэтому порядок моделей и количество обучающих данных были ограничены. 

Для расчета ARIMA-моделей была использована Java-библиотека с открытым 
исходным кодом -- Workday/timeseries-forecast. На одних и тех же входных 
данных по загрузке процессора, собранных за один час работы системы 
(процесс сбора данных описан в предыдущем разделе), были построены модели 
с порядками $p \le 11$, $d \le 3$ и $q \le 11$. Количество обучающих данных $n$ 
варьировалось от 10 до 4000. 

Оценка точности модели производилась по следующему алгоритму: 
\begin{itemize}
	\item рассчитывалась ARIMA модель с порядками $p$, $d$ и $q$ на 
		последовательных значениях загрузки с $i$-го по $i + n - 1$ 
	\item рассчитывался прогноз $f$ следующего шага -- $i + n$
	\item происходила нормализация спрогнозированного значения по
		следующему правилу: 
				если $f < 0$, то $f_n = 0$; если $f > 1$, то $f_n = 1$
	\item фиксировалась ошибка -- разница между фактическим значением 
		временного ряда и нормализованным значением $f_n$
	\item выполненные действия повторялись при $i = i + 1$
\end{itemize}
После завершения работы данного алгоритма вычислялась средняя 
квадратическая ошибка (root mean square error RMSE, данный способ оценки 
точности часто используется для сравнения моделей пронозирования 
\cite{liu1994, mills2011, hyndman2012}) модели порядков $p$, $d$, $q$ с 
количеством обучающих данных равным $n$. Результаты, отсортированные в 
порядке возрастания ошибки, представлены в таблицах 
\labelcref{tab:mamodels,tab:armodels,tab:armamodels,tab:arimamodels}. 
Результаты прогнозов наиболее точных моделей AR, MA, ARMA и ARIMA 
изображены на рисунке \ref{fig:arimaforecasting}.

\begin{gosttable}
	\caption{Сравнение моделей MA}
	\begin{tabular}{| M{3.5cm} | M{3cm} | M{3.5cm} |}
		\hline
		Порядок модели MA ($q$) 
			& Количество обучающих данных ($n$)
			& Cредняя квадратическая ошибка \\ 
		\hline
		1 & 15 & 0.116628 \\ \hline
		1 & 16 & 0.116784 \\ \hline
		1 & 11 & 0.116977 \\ \hline
		1 & 13 & 0.117005 \\ \hline
		1 & 18 & 0.117108 \\ \hline
		1 & 20 & 0.117462 \\ \hline
		1 & 19 & 0.117549 \\ \hline
		1 & 14 & 0.117555 \\ \hline
		1 & 17 & 0.117649 \\ \hline
		1 & 22 & 0.117656 \\ \hline
	\end{tabular}
	\label{tab:mamodels}
\end{gosttable}

\begin{gosttable}
	\caption{Сравнение моделей AR}
	\begin{tabular}{| M{3.5cm} | M{3cm} | M{3.5cm} |}
		\hline
		Порядок модели AR ($p$) 
			& Количество обучающих данных ($n$)
			& Cредняя квадратическая ошибка \\ 
		\hline
		9 & 800 & 0.103868 \\ \hline
		8 & 800 & 0.103885 \\ \hline
		8 & 700 & 0.103948 \\ \hline
		9 & 700 & 0.103956 \\ \hline
		10 & 800 & 0.103992 \\ \hline
		10 & 700 & 0.104136 \\ \hline
		7 & 800 & 0.104277 \\ \hline
		9 & 900 & 0.104283 \\ \hline
		8 & 900 & 0.104333 \\ \hline
		7 & 700 & 0.104349 \\ \hline
	\end{tabular}
	\label{tab:armodels}
\end{gosttable}

\begin{gosttable}
	\caption{Сравнение моделей ARMA}
	\begin{tabular}{| M{3cm} | M{3cm} | M{3.5cm} |}
		\hline
		Порядки модели ARMA ($p, q$) 
			& Количество обучающих данных ($n$)
			& Cредняя квадратическая ошибка \\ 
		\hline
		8, 1 & 800 & 0.104518 \\ \hline
		9, 1 & 800 & 0.104555 \\ \hline
		10, 1 & 700 & 0.104639 \\ \hline
		9, 1 & 700 & 0.104718 \\ \hline
		8, 1 & 700 & 0.104766 \\ \hline
		10, 1 & 900 & 0.104776 \\ \hline
		10, 1 & 800 & 0.104785 \\ \hline
		9, 2 & 700 & 0.104795 \\ \hline
		9, 3 & 800 & 0.104845 \\ \hline
		9, 1 & 900 & 0.104894 \\ \hline
	\end{tabular}
	\label{tab:armamodels}
\end{gosttable}

\begin{gosttable}
	\caption{Сравнение моделей ARIMA}
	\begin{tabular}{| M{4cm} | M{3cm} | M{3.5cm} |}
		\hline
		Порядки модели ARIMA ($p, d, q$) 
			& Количество обучающих данных ($n$)
			& Cредняя квадратическая ошибка \\ 
		\hline
		0, 1, 3 & 700 & 0.103599 \\ \hline
		7, 1, 0 & 700 & 0.103604 \\ \hline
		8, 1, 0 & 700 & 0.103613 \\ \hline
		8, 1, 0 & 800 & 0.103665 \\ \hline
		7, 1, 0 & 800 & 0.103714 \\ \hline
		7, 1, 0 & 600 & 0.103752 \\ \hline
		0, 1, 3 & 800 & 0.103774 \\ \hline
		8, 1, 0 & 600 & 0.103777 \\ \hline
		10, 1, 0 & 800 & 0.103798 \\ \hline
		9, 1, 0 & 700 & 0.103804 \\ \hline
	\end{tabular}
	\label{tab:arimamodels}
\end{gosttable}
\vspace{1em}
\begin{figure}[!h]
	\centering
	\includegraphics[width=\linewidth]{\detokenize{arima_forecasting}}
	\caption{Прогноз загрузки процессора}
	\label{fig:arimaforecasting}
\end{figure} 

\newpage
На рисунке \ref{fig:arimaforecasting} можно отметить, что резкие скачки нагрузки 
не прогнозируются ни одной из моделей. Кроме того наиболее точные модели 
вычисляют повторение исходного ряда, но смещенного по времени. По сути 
значения рассчитанные любой из вышеперечисленных моделей нельзя считать 
прогнозом. Совокупность данных фактов не соответствует основному 
требованию прогнозирования пиков избыточной нагрузки, приведённому в 
начале раздела.

\subsection{Сравнение нейросетевых моделей} \label{subsec:nnmodelscompare}
Ранее в главе были представлены два типа нейронных сетей, используемых для 
прогнозирования временных рядов. Следует выделить их особенности 
\cite{chernodub2012}:
\begin{enumerate}
	\item При использовании многослойного персептрона, в отличие от 
		рекуррентного, необходимо заранее знать порядок линии задержек на 
		входе. Однако, в пределах данной работы это не является проблемой, 
		так как порядок может быть выбран во время экспериментов, и 
		больше не меняться в ходе работы системы.
	\item Рекуррентные сети имеют б\'{о}льшую точность при многошаговом 
		прогнозировании, но это так же не имеет значения в данной работе.
	\item Обучение рекуррентных сетей является более трудоемкой задачей, в 
		виду наличия у них дополнительных степеней свободы 
		\cite{medsker2000, naseera2015}.
\end{enumerate}
Из этого следует, что при выборе между двумя типами нейронных сетей 
наибольший интерес (на основании теоретических сведений) представляет 
первый. 

Но наличие рекуррентных связей является не единственной структурной 
конфигурацией, приводящим к изменению точности прогнозирования. Влияние 
оказывают и другие параметры, такие как: количество слоёв, количество  
нейронов в скрытых слоях, а так же активационные функции нейронов. Более  
того, при изменении алгоритма обучения можно добиться различной 
сходимости, а следовательно -- и точности \cite{naseera2015}.

Для составления и обучения нейронной сети был использован Java"=фреймворк 
с открытым исходным кодом -- Encog. В качестве обучающего был взят тот же 
участок данных, что использовался при построении моделей ARIMA в 
предыдущем разделе. На вход нейронной сети подавались значения семнадцати 
динамических характеристик в момент времени $t$, а также $n - 1$ предыдущих 
значений каждой из характеристик для сети с линией задержек порядка $n$ 
(более подробное описание данных характеристик представлено в подразделе 
\ref{subsec:datapreparation}). Если при $t + 1$ система находилась в состоянии 
перегрузки, то на выходе сети ожидалось значение 1. В обратном случае -- 0.

Всего было обучено около десяти тысяч нейронных сетей различных 
конфигураций. При этом количество скрытых слоёв варьировалось от одного до 
трёх, а количество нейронов в скрытых слоях от одного до количества нейронов 
предыдущего слоя. Кроме того, изменялись функции активации скрытого и 
выходного слоёв (всего использовалось 3 типа функций: сигмоидальная, 
гиперболического тангенса и логарифмическая), а так же порядок линии 
задержек от одного до десяти для нейронной сети с линией задержек. При 
обучении использовался метод Resilient Propagation (упругого распространения), 
как показавший наибольшую точность при решении подобных задач 
\cite{naseera2015}. Обучение продолжалось до двадцати тысяч эпох. Однако при 
обучении рекуррентной нейронной сети не удалось добиться схождения.

Суммарно на обучение нейронных сетей было затрачено около трёх недель. В 
виде тестовых данных был взят еще один участок статистических данных 
длиной один час. Точность оценивалась по факту значения ошибки -- числа 
неверно спрогнозированных бинарных значений относительно общего числа 
прогнозов на обучающих и тестовых данных. Десять конфигураций сетей 
отсортированные в порядке возрастания ошибки на обучающих данных и
соответствующие требованиям точности предсказания пиков избыточной 
нагрузки представлены в таблице \ref{tab:nnmodels}.

\begin{gosttable}
	\caption{Сравнение нейросетевых моделей}
	\begin{tabular}{| M{.113\textwidth} | M{.17\textwidth} | M{.114\textwidth} |
				M{.143\textwidth} | M{.133\textwidth} | M{.23\textwidth} |}
		\hline
		\multicolumn{3}{| c |}{Конфигурация нейронной сети} 
			& \multirow{4}{*}{\parbox[c]{0.143\textwidth}
						{\centering Ошибка на обучающей выборке, \%}} 
			& \multirow{4}{*}{\parbox[c]{0.133\textwidth}
						{\centering Ошибка на тестовых данных, \%}} 
			& \multirow{4}{*}{\parbox[c]{0.23\textwidth}
						{\centering Ошибка предсказания пиков избыточной 
									нагрузки на тестовых данных, \%}} \\
		\cline{1-3}
		Порядок линии задержек 
			& Количество нейронов в 1-м, 2-м и 3-м слоях 
			& Функция активации & & & \\
		\hline
		8 & 80, 48, 16 & $log$ & 0.08 & 2.53 & 1.86 \\ \hline
		7 & 63, 35, 14 & $log$ & 0.13 & 2.22 & 2.05 \\ \hline
		7 & 70, 28, 14 & $log$ & 0.14 & 2.31 & 2.05 \\ \hline
		8 & 72, 40, 8 & $log$ & 0.14 & 2.42 & 2.79 \\ \hline
		6 & 60, 24, 6 & $log$ & 0.14 & 2.79 & 2.61 \\ \hline
		7 & 70, 35, 7 & $log$ & 0.15 & 2.43 & 2.42 \\ \hline
		6 & 60, 24, 18 & $log$ & 0.21 & 2.28 & 3.35 \\ \hline
		6 & 54, 24, 18 & $log$ & 0.27 & 2.11 & 4.10 \\ \hline
		6 & 60, 30, 6 & $log$ & 0.29 & 2.16 & 4.84 \\ \hline
		7 & 56, 35, 7 & $log$ & 0.35 & 2.62 & 4.84 \\ \hline
	\end{tabular}
	\label{tab:nnmodels}
\end{gosttable}


\section{Результаты сравнительного анализа} 
В результате сравнительно анализа были сделаны следующие выводы: 
\begin{enumerate}
	\item Модели AR, MA, ARMA, ARIMA не подходят для прогнозирования 
		нагрузки процессора, в силу недостаточной точности на данном виде 
		временного ряда.
	\item Ошибка прогнозирования пиков избыточной нагрузки части моделей 
		прогнозирования, составленных на базе многослойного пересептрона, 
		меньше 5 \% на обучающей выборке, что соответствует требованиям. 
		Однако при разработке алгоритма распределения нагрузки необходимо 
		расширить обучающую выборку.
\end{enumerate}

Так же были составлены следующие рекомендации по выбору моделей 
прогнозирования:
\begin{itemize}
	\item При необходимости прогнозирования поведения рядов, имеющих 
		плавный и непрерывистый характер, наиболее подходящим выбором 
		будут модели класса ARIMA. 
	\item Если при этом ряд стационарен, то будет достаточно класса ARMA.
	\item При выборе модели класса ARIMA в первую очередь необходимо 
		проверить точность моделей низших порядков.
	\item Если необходимо автоматизировать процесс выбора модели поведения 
		на основании значений одномерного или многомерного временного 
		ряда, то в таком случае подойдут модели составленные на базе 
		многослойного персептрона.
	\item Если требуется модель, вычисляющая многошаговый прогноз, то 
		наиболее подходящими являются модели, составленные на базе 
		рекуррентного многослойного персептрона.
\end{itemize}

\chapter{Разработка алгоритма распределения нестационарной нагрузки}
\section{Необходимые функциональные возможности}
Для разрабатываемого алгоритма были определены необходимые 
функциональные возможности, а именно: 
\begin{enumerate}
	\item Формирование решения о необходимости приостановки запуска 
		новых загрузчиков внешних ресурсов и передачи пользователям 
		данных из кэша, на основании предоставленных алгоритму 
		статистических данных. При таком подходе система не будет тратить 
		свои вычислительные ресурсы на загрузку новых данных из внешних 
		ресурсов, что увеличит её производительность и уменьшит среднее 
		время ответа. 
	\item Возможность продолжения обучения лежащей в основе алгоритма 
		нейронной сети. За расширением обучающей выборки и продолжением 
		обучения нейронной сети следует увеличение точности определения 
		перегрузок вычислительной системы, что значительно улучшает 
		эффективность работы алгоритма.
\end{enumerate}

\section{Требования к алгоритму}
Так как алгоритм должен быть встроен в разрабатываемый сервис, были 
сформированы следующие требования:
\begin{enumerate}
	\item Практическая реализация разработанного решения должна быть 
		осуществлена на языке Java SE 8, так как серверная часть 
		разрабатываемого сервиса уже реализована на языке Java версии 8 и 
		это обеспечит простую встраиваемость.
	\item Алгоритм должен быть представлен в виде компонента Spring 
		Framework, чтобы обеспечить единообразность кода.
	\item Обученная нейронная сеть, а так же её обучающая выборка должны 
		храниться на диске, в силу того, что операции чтения или записи этих 
		данных при стабильной работе сервиса будут происходить не больше  
		двух -- трёх раз в сутки.
	\item Все настройки алгоритма должны задаваться при помощи файлов 
		свойств Spring Framework (по умолчанию application.properties или 
		application.yml), так как все конфигурационные параметра указываются 
		именно таким способом.
\end{enumerate}

\section{Разработка алгоритма} \label{sec:algdevelopment}
После внедрения алгоритма распределения запросов поведение 
разрабатываемого сервиса изменится, в силу следующих причин:
\begin{enumerate}
	\item Каждые 300 мс. вместе с получением новых данных будет 
		рассчитываться прогноз, а точнее производиться перемножение 
		весовых коэффициентов каждого слоя нейронной сети на значения 
		полученные из функций активации предыдущих слоев. Это займет 
		часть процессорного времени вычислительной системы.
	\item Если нейронная сеть спрогнозирует перегрузку системы, то 
		результаты поиска не будут загружаться из внешних ресурсов и 
		пользователи сразу же получат результаты найденные в кэше. В 
		таком случае нагрузка системы снизится. 
\end{enumerate}
Поэтому данные необходимые для обучения нейронной сети могут быть 
получены только при её функционировании в составе сервиса. Тогда 
на вход нейронной сети с линией задержек порядка $n$ так же может 
подаваться восемнадцатый временной ряд -- предыдущие $n$ бинарных 
значений с выхода нейронной сети. Анализ данного ряда позволит определить 
снижение нагрузки, которое может оказать влияние на прогноз поведения 
системы. 

Для решения проблемы недостаточности обучающей выборки нейронной сети, 
описанной в конце подраздела \ref{subsec:nnmodelscompare}, был разработан 
алгоритм аккумулирования обучающей выборки, который позволил не 
перегружать обучающую выборку большим объемом данных, и в то же время 
постепенно увеличивать точность прогнозирования посредством её дополнения и 
повторного обучения нейронной сети. Алгоритм представлен ниже:
\begin{enumerate}
	\item Формирование тестовой выборки из собранных статистических 
		данных за прошедшие $n$ дней.
	\item \label{alg:l1} Расчет прогноза нейронной сетью на тестовой выборке.
	\item Дополнение обучающей выборки, сохранённой после предыдущего 
		запуска данного алгоритма, при ошибочном прогнозе (либо пустой 
		обучающей выборки, если алгоритм запущен впервые). 
	\item Обучение нейронной сети на дополненной обучающей выборке.
	\item \label{alg:l2} Фиксирование в памяти обученной нейронной сети, её 
		обучающей выборки и точности на тестовой выборке.
	\item Если не израсходовано время отведённое на отработку алгоритма, 
		переход к пункту \ref{alg:l1}.
	\item Сохранение нейронной сети с наибольшей точностью, а так же её 
		обучающей выборки.
\end{enumerate}
Данный алгоритм может запускаться в определенные часы ночью, пока сервис 
простаивает.

После встраивания обоих алгоритмов в разрабатываемый сервис, было 
произведено дополнительное сравнение пяти конфигураций нейронных сетей с 
наибольшей точностью из подраздела \ref{subsec:nnmodelscompare}. В связи с 
добавлением восемнадцатой характеристики, количество нейронов скрытых 
слоёв было увеличено пропорционально увеличению числа нейронов входного 
слоя. Для сравнения использовался модифицированный тестовый стенд из 
подраздела \ref{subsec:datapreparation}, представленный на рисунке 
\ref{fig:standafter}, где для работы распределителя запросов пользователей 
использовался разработанный алгоритм распределения запросов. При сравнении 
был использован тот же тестовый план для Apache JMeter, что и в подразделе 
\ref{subsec:datapreparation}. С целью уменьшения времени затрачиваемого на сбор 
данных и обучение нейронных сетей, было решено сократить время сбора данных 
с дневного интервала до четырёх часов. Чтобы проверить обучаемые сети на 
сходимость, обучение продолжалось до тех пор, пока ошибка на обучающей 
выборке не опускалась до величины меньшей чем 0.5 \%. Сбор данных и 
последующее за ним обучение проводились пять раз.

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{\detokenize{stand_after}}
	\caption{Схема тестового стенда}
	\label{fig:standafter}
\end{figure} 

Первая нейронная сеть была обучена тремя способами: 
\begin{enumerate}
	\item Тестовая выборка состояла из статистических данных собранных за 
		последние четыре часа работы сервиса, или один временной 
		промежуток сбора данных. Ошибка прогнозирования полученной в 
		результате обучения сети составила 0.67 \%, а суммарное время 
		обучения — 35 часов 12 минут.
	\item Тестовые данные были собраны за восемь часов работы сервиса, или 
		два временных промежутка сбора данных. Ошибка прогнозирования: 
		0.54 \%, суммарное время обучения: 28 часов 27 минут.
	\item Три временных промежутка сбора данных. Ошибка прогнозирования: 
		0.48 \%, суммарное время обучения: 24 часа.
\end{enumerate}
В результату сравнения трёх способов обучения, для обучения остальных 
конфигураций нейронных сетей был выбран второй способ, так как он дает 
значительный прирост скорости обучения и точности. Процент ошибки 
уменьшился на 0.13 \%, а время обучения на 6 часов 45 минут, (или на 19.4 \% и 
19.2 \% относительно исходной величины). 

На рисунке \ref{fig:nettraining} можно увидеть графики обучения данной сети 
вторым способом. В легенде указано на каком промежутке данных производится 
расчёт ошибки прогнозирования (как описано в подразделе 
\ref{subsec:datapreparation}: сбор статистических данных производится раз в 300 
мс., поэтому участок $(1-48000)$ соответствует четырёх часовому периоду). Одна 
итерация -- это выполнения пунктов \ref{alg:l1}--\ref{alg:l2} аккумулирующего 
алгоритма. Сплошными линиями на графике обозначен процент ошибки на 
тестовой выборке; пунктирными -- на данных, ранее побывавших в тестовой 
выборке.

\begin{figure}[h]
	\centering
	\includegraphics[width=\linewidth]{\detokenize{net_training}}
	\caption{Графики итерационного обучения нейронной сети при помощи 
							разработанного аккумулирующего алгоритма}
	\label{fig:nettraining}
\end{figure} 

Из графиков видно, что процесс обучения нейронной сети сходится, а процент 
ошибки на предшествующих тестовых данных к концу обучения не превысил 
1 \%.  При наличии ограничения на время обучения, процент ошибки будет 
постепенно уменьшаться за счет выбора наиболее точной сети.

Всего на сбор данных и обучение нейронных сетей было затрачено около 
двенадцати дней. Быстрее остальных была обучена сеть с линией задержек 
шестого порядка, 64 нейронами в первом скрытом слое, 26 во втором и 19 в 
третьем. Её обучение заняло 25 часов 36 минут, а размер обучающей выборки 
составил 19542 (примерно 98 минут, или 8.1 \% от общего количества данных). 
Данная нейронная сеть была задействована в конечном варианте 
разработанного алгоритма распределения нестационарной нагрузки.

\section{Практическая реализация разработанного алгоритма}
В итоге разработанный алгоритм реализует 3 функции:
\begin{itemize}
	\item {\itshape double getError();}  \\
		Возвращает процент ошибки нейронной сети на тестовых данных, 
		рассчитанный во время последнего обучения.
	\item {\itshape boolean isOverloaded();}  \\
		Принимает на вход массив из восемнадцати статистических 
		характеристик и пяти задержанных во времени значений для каждой. 
		Возвращает true, если система нуждается в уменьшении нагрузки.
	\item {\itshape void startTraining();} \\ 
		Принимает на вход тестовую выборку, на которой производится 
		обучение сети согласно алгоритму аккумуляции, представленному в 
		предыдущем разделе. Время обучения ограничивается параметром, 
		описанным ниже.
\end{itemize}

Для составления, сохранения в файл, загрузки из файла и обучения нейронной 
сети используется Encog Framework.

При помощи файлов свойств Spring Framework могут быть заданы следующие 
параметры разработанного алгоритма:
\begin{itemize}
	\item \url{forecasting.path-to-stored-data} \\ 
		При помощи данного параметра задаётся путь к хранимым алгоритмом 
		данным.
	\item \url{forecasting.learning-limit} \\ 
		Задаёт ограничение на время обучения сети.
\end{itemize}

После каждого обучения сеть, её точность и обучающая выборка сохраняются в 
файлы, расположенные по указанному пути. При запуске сервиса происходит 
загрузка нейронной сети и точности из файлов. Если файл с нейронной сетью не 
обнаружен или поврежден -- создаётся новая нейронная сеть. Если значение 
ошибки на тестовых данных неизвестно, то функцией getError() будет 
возвращено значение NaN. 


\chapter{Тестирование разработанного алгоритма}
\section{Тестовое окружение}
Для сравнения среднего времени ответа модифицированного и исходного 
сервисов были использованы тестовые стенды (рисунок \ref{fig:standbefore} и 
рисунок \ref{fig:standafter}) представленные в подразделе 
\ref{subsec:datapreparation} и разделе \ref{sec:algdevelopment}. Так же были 
сгенерированы два расписания активных потоков, каждый длительностью 24 
часа, с максимальным числом потоков равным 560, и 460 (более подробно 
процесс генерации описан в подразделе \ref{subsec:datapreparation}). Так как в 
первом случае число активных потоков доходило до 560, то при отсутствии 
алгоритма распределения запросов система 37 \% времени тестирования 
находилась в состоянии перегрузки. Во втором случае, в виду меньшего 
количества активных потоков, система находилась в состоянии перегрузки чуть 
более 1 \% времени. Наличие двух видов тестов позволяет проверить какое 
влияние оказывает алгоритм распределения на поведение сервиса при наличии 
и отсутствии перегрузок.

Каждый из двух вариантов тестов проводился по четыре раза на сервисе с 
распределителем нагрузки и без него. Измерения времени ответа производились 
средствами Apache JMeter. 

\section{Результаты тестирования}
После проведения тестов было рассчитано среднее время ответа для каждого из 
видов запросов. Так же была рассчитана величина 95-го перцентиля для первого 
теста и 99-го для второго \cite{eliseeva2004}. Кроме того, для величин среднего 
времени ответа был так же рассчитан доверительный интервал с уровнем 
доверия 95 \%  \cite{eliseeva2004}.

Результаты тестирования представлены в таблицах \ref{tab:test1} и \ref{tab:test2}. 
Для обоих тестов в отдельном столбце произведено сравнение среднего времени 
ответа и перцентелей для исходного сервиса и сервиса с разработанным 
алгоритмом.

\begin{gosttable}
	\caption{Результаты первого тестирования}
	\begin{tabular}{| M{.06\textwidth} | M{.1\textwidth} | M{.2\textwidth} | 
			M{.19\textwidth} | M{.019\textwidth} | M{.19\textwidth} | }
		\cline{1-4} \cline{6-6}
		\multicolumn{2}{| r |}{\parbox[c]{.1\textwidth}{\centering Вид запроса}} 
			& Исходный сервис 
			& Сервис с алгоритмом распределения
			&& Уменьшение величины, \% \\ \cline{1-4} \cline{6-6}
		\multirow{6}{*}{\rotatebox[origin=c]{90}{\parbox[c]{5cm}
			{\centering Среднее время ответа, мс}}} 
		& 1 & 133.5 $\pm$ 4.5 & 112.6 $\pm$ 3.6 && 15.7 \\ \cline{2-4} \cline{6-6}
		& 2 & 152.9 $\pm$ 5.3 & 120.3 $\pm$ 4.3 && 21.3 \\ \cline{2-4} \cline{6-6}
		& 3 & 122.5 $\pm$ 5.7 & 98.7 $\pm$ 4.1 && 19.4 \\ \cline{2-4} \cline{6-6}
		& 4 & 23.5 $\pm$ 2.7 & 21.1 $\pm$ 1.5 && 10.2 \\ \cline{2-4} \cline{6-6}
		& 5 & 72.3 $\pm$ 4.2 & 57.8 $\pm$ 3.3 && 20.1 \\ \cline{2-4} \cline{6-6}
		& 6 & 70.6 $\pm$ 4.4 & 57.1 $\pm$ 3.5 && 19.1 \\ \cline{1-4} \cline{6-6}
		
		\multirow{6}{*}{\rotatebox[origin=c]{90}{\parbox[c]{5cm}
			{\centering 95-й перцентиль, мс}}} 
		& 1 & 552 & 438 && 20.7 \\ \cline{2-4} \cline{6-6}
		& 2 & 612 & 436 && 28.8 \\ \cline{2-4} \cline{6-6}
		& 3 & 581 & 440 && 24.3 \\ \cline{2-4} \cline{6-6}
		& 4 & 106 & 91 && 14.2 \\ \cline{2-4} \cline{6-6}
		& 5 & 278 & 204 && 26.6 \\ \cline{2-4} \cline{6-6}
		& 6 & 270 & 212 && 21.5 \\ \cline{1-4} \cline{6-6}
	\end{tabular}
	\label{tab:test1}
\end{gosttable}

\begin{gosttable}
	\caption{Результаты второго тестирования}
	\begin{tabular}{| M{.06\textwidth} | M{.1\textwidth} | M{.2\textwidth} | 
			M{.19\textwidth} | M{.019\textwidth} | M{.19\textwidth} | }
		\cline{1-4} \cline{6-6}
		\multicolumn{2}{| r |}{\parbox[c]{.1\textwidth}{\centering Вид запроса}} 
			& Исходный сервис 
			& Сервис с алгоритмом распределения
			&& Уменьшение величины, \% \\ \cline{1-4} \cline{6-6}
		\multirow{6}{*}{\rotatebox[origin=c]{90}{\parbox[c]{5cm}
			{\centering Среднее время ответа, мс}}} 
		& 1 & 38.0 $\pm$ 1.1 & 37.1 $\pm$ 1.1 && 2.3 \\ \cline{2-4} \cline{6-6}
		& 2 & 34.4 $\pm$ 1.1 & 33.8 $\pm$ 1.0 && 1.8 \\ \cline{2-4} \cline{6-6}
		& 3 & 26.1 $\pm$ 0.9 & 25.8 $\pm$ 0.8 && 1.1 \\ \cline{2-4} \cline{6-6}
		& 4 & 12.7 $\pm$ 0.5 & 12.6 $\pm$ 0.5 && 0.8 \\ \cline{2-4} \cline{6-6}
		& 5 & 26.0 $\pm$ 1.7 & 25.3 $\pm$ 1.5 && 2.7 \\ \cline{2-4} \cline{6-6}
		& 6 & 20.3 $\pm$ 1.3 & 19.9 $\pm$ 1.2 && 2.0 \\ \cline{1-4} \cline{6-6}
		
		\multirow{6}{*}{\rotatebox[origin=c]{90}{\parbox[c]{5cm}
			{\centering 99-й перцентиль, мс}}} 
		& 1 & 110 & 99 && 10.0 \\ \cline{2-4} \cline{6-6}
		& 2 & 104 & 95 && 8.7 \\ \cline{2-4} \cline{6-6}
		& 3 & 88 & 83 && 5.7 \\ \cline{2-4} \cline{6-6}
		& 4 & 35 & 34 && 2.8 \\ \cline{2-4} \cline{6-6}
		& 5 & 115 & 103 && 10.4 \\ \cline{2-4} \cline{6-6}
		& 6 & 101 & 91 && 9.9 \\ \cline{1-4} \cline{6-6}
	\end{tabular}
	\label{tab:test2}
\end{gosttable}

\section{Выводы по результатам тестирования}
По результатам тестирования можно сделать вывод, что в случае появления 
значительных перегрузок системы, разработанный алгоритм, при встраивании 
его в сервис"=агрегатор, обеспечивает в среднем для всех видов запросов на 
17.6 \% меньшее время ответа, а так же уменьшение 95-го перцентиля на 22.7 \%. 
Следует отметить, что работа алгоритма оказывает положительное влияние на 
время ответа для всех видов запросов без исключения.

Если же система не подвержена постоянным перегрузкам, то алгоритм 
обеспечивает уменьшение величины среднего времени ответа на 1.1 \% и 99-го 
перцентиля на 7.9 \%. 

Уменьшение данных характеристик в обоих тестах является показателем того, 
что алгоритм справляется с поставленными задачей предупреждения перегрузок 
вычислительной системы. 

Стоит отметить, что полученные результаты не являются конечными. При 
продолжении обучения нейронной сети, лежащей в основе разработанного 
алгоритма, точность, а с ней и эффективность алгоритма могут быть улучшены.

\likechapter{Заключение}
По завершению работы были достигнуты следующие результаты.
\begin{enumerate}
	\item Выполнен сравнительный анализ методов прогнозирования.
	\item Сформулированы рекомендации по выбору модели прогнозирования.
	\item Предложен алгоритм аккумуляции обучающей выборки нейронной 
		сети.
	\item Разработан алгоритм распределения нестационарной нагрузки.
	\item Выполнено сравнение исходной, и использующей разработанный 
		алгоритм систем.
\end{enumerate}

%\printbibliography[heading=bibheading]
\clearpage
\addtocontents{toc}{\vspace{1em}}
\addcontentsline{toc}{likechapter}{\MakeUppercase{Библиографический список}}
\bibliography{references/literature}
 
\end{document}
